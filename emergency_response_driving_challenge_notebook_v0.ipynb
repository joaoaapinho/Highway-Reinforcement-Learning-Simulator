{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning & Reinforcement Learning\n",
    "# **Group Assignment**\n",
    "### **Highway Environment: Emergency Response Driving Challenge**\n",
    "Professor: David Kremer\n",
    "\n",
    "Assignment done by: \n",
    "- Adrian Marino\n",
    "- Felipe Fischel\n",
    "- Gilles Calderón\n",
    "- Jean-Jacon Klat\n",
    "- João André Pinho\n",
    "- Nicholas Dieke\n",
    "- Niccolo Borgato\n",
    "- Max Heilingbrunner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Resolution**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part 0: Imports & Setup**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Libraries and Module Imports:**\n",
    "\n",
    "Importing the necessary libraries and modules to manipulate and interact with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import matplotlib.animation as animation\n",
    "from pyvirtualdisplay import Display\n",
    "import gym.wrappers\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install highway-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyvirtualdisplay highway-env stable_baselines3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Video Replay Setup:**\n",
    "\n",
    "Creating a path for storing gameplay animations from the different tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a directory to save the video files\n",
    "video_dir = '/path/to/video_dir'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part I: Setting Up the Environment and Action Space**\n",
    "\n",
    "Define the environment and action space that will be used - \"Highway-v0\" and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the type of environment.\n",
    "environment_name = 'highway-v0'\n",
    "\n",
    "# Creating an instance of it and setting the render mode as \"human\" so the environment can be seen on video.\n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'type': 'DiscreteMetaAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 40,\n",
      " 'ego_spacing': 2,\n",
      " 'high_speed_reward': 0.4,\n",
      " 'initial_lane_id': None,\n",
      " 'lane_change_reward': 0,\n",
      " 'lanes_count': 4,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'type': 'Kinematics'},\n",
      " 'offroad_terminal': False,\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 1,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'reward_speed_range': [20, 30],\n",
      " 'right_lane_reward': 0.1,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15,\n",
      " 'vehicles_count': 50,\n",
      " 'vehicles_density': 1}\n"
     ]
    }
   ],
   "source": [
    "# Checking the current environment configuration.\n",
    "\n",
    "pprint.pprint(env.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Change the Action Space:**\n",
    "\n",
    "Changing the action space from discrete to continuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Default Action Space.\n",
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** : As of now, the default action space uses 5 different actions:\n",
    " \n",
    " - **Lane Left**\n",
    " - **Idle**\n",
    " - **Lane Right**\n",
    " - **Faster**\n",
    " - **Slower**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.9039406 ,  0.75      ,  0.3125    ,  0.        ],\n",
       "        [ 1.        ,  0.10145508, -0.5       , -0.03188379,  0.        ],\n",
       "        [ 1.        ,  0.20449375, -0.75      , -0.04452832,  0.        ],\n",
       "        [ 1.        ,  0.30793422, -0.75      , -0.04818792,  0.        ],\n",
       "        [ 1.        ,  0.42262217, -0.5       , -0.01568883,  0.        ]],\n",
       "       dtype=float32),\n",
       " {'speed': 25,\n",
       "  'crashed': False,\n",
       "  'action': array([-0.09063949, -0.3238328 ], dtype=float32),\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'right_lane_reward': 1.0,\n",
       "   'high_speed_reward': 0.5,\n",
       "   'on_road_reward': 1.0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the Default Action Space of the environment to Continuos, in order to allow the agent to directly set the low-level controls of the vehicle kinematics.\n",
    "# This will allow the agent to change its steering angle and throttle (acceleration).\n",
    "env.configure({\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\"\n",
    "    }\n",
    "})\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (2,), float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the New Action Space.\n",
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** : The new action space will be continuos with 2 different actions:\n",
    " \n",
    " - **Steering -** [-1 Full Left, +1 Full Right]\n",
    " - **Throttle -** [-1 Full Brake, +1 Full Acceleration]\n",
    "\n",
    " This change was performed in order for the car to be able to have a smoother and finer control over its actions, which can be beneficial in complex and dynamic environments like a highway."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Change the Observation Space:**\n",
    "\n",
    "Changing the observation space from array-based to image-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (5, 5), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Default Observation Space.\n",
    "env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The default Observation Space consists of a 2D 5x5 array, where each row corresponds to one vehicle in this environment and the columns correspond to different characteristics of that vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Changing the Observation Space to a Grayscale Image based one.\n",
    "%matplotlib inline\n",
    "config = {\n",
    "   \"observation\": {\n",
    "       \"type\": \"GrayscaleObservation\",\n",
    "       \"observation_shape\": (128, 64),\n",
    "       \"stack_size\": 4,\n",
    "       \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "       \"scaling\": 1.75,\n",
    "   },\n",
    "   \"policy_frequency\": 2\n",
    "}\n",
    "\n",
    "env.configure(config)\n",
    "obs, info = env.reset()\n",
    "\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "   ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (4, 128, 64), uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Default Observation Space.\n",
    "env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The new Observation Space will consist of a 128x64 Grayscale four image stack from the Gameplay, where each image can take on a value from 0 (indicating black) to 255 (indicating white).\n",
    "\n",
    "This change was performed in order to train a model that would present more similarity with the real-world data. As autonomous driving vehicles primarily rely on visual input from cameras, training an RL agent with similar image-based inputs can make the learned policies more directly tranferable to a real-world scenario. Additionally, as images encapsulate a wider array of spacial details, such as positioning of surrounding vehicles, lane markings, and other environmental features, the model may gain a more comprehensive and valuable understanding of the scene, facilitating its learning and precise decion making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.wrappers' has no attribute 'Monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Wrapping the environment with Monitor wrapper to save the gameplays.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mwrappers\u001b[39m.\u001b[39;49mMonitor(env, video_dir, force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym.wrappers' has no attribute 'Monitor'"
     ]
    }
   ],
   "source": [
    "# Wrapping the environment with Monitor wrapper to save the gameplays.\n",
    "env = gym.wrappers.Monitor(env, video_dir, force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part II: Running a Baseline of Random Episodes**\n",
    "\n",
    "Let the agent take random actions in the environment in order to give a baseline of what to expect from an untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AbstractEnv.render() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     15\u001b[0m \u001b[39m# Adding the first frame.\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m frames\u001b[39m.\u001b[39mappend(env\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     18\u001b[0m \u001b[39m# Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:65\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: AbstractEnv.render() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "# Defining the number of episodes - number of times the game will play itself either if:\n",
    "#   A. The car visits all tiles.\n",
    "#   B. Goes outside of the playfield and far from the track.\n",
    "episodes = 5\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    \n",
    "    # Reseting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "    \n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0 \n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "        \n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "    \n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As seen, without properly training the agent to appropriatly define the right actions to take the accumulated final scores will be very poor (negative values). \n",
    "\n",
    "In order to improve these the agent must be trained in order to:\n",
    "\n",
    "1. **Gather samples from the gameplay (i.e., running the policy);**\n",
    "2. **Fit a model that estimates the tota final return;**\n",
    "3. **Improving the policy, in order to generate at each step an action with the end goal of maximizing the final acuumulated reward.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part III: Setting up the Agent & Policy**\n",
    "\n",
    "Set up the Agent & Policy using Stable Baselines3 - a famous high-level Python library for reinforcement learning exercises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the environment.\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "# Wraping the environment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Setup the Agent and Policy**\n",
    "\n",
    "Creating an agent using the **Proximal Policy Optimization** algorithm.\n",
    "\n",
    "This algorithm was built in order to optimize the decisions made by an agent in a given environment in order to maximize the notion of cumulative reward. The idea behind consists of taking steps in the direction that improves the policy, but not too large ones that might harm the performance. In other words, it seeks an optimal policy that is close to the current policy. \n",
    "\n",
    "Mathematically, PPO aims to find the optimal policy π(θ) that maximizes the objective:\n",
    "<center><img src=\"images/ppo_formula.png\"/></center>\n",
    "\n",
    "The main innovation of the this algorithm is the introduction of a \"Clipping\" mechanism that penalizes changes in the policy that deviate too much from the curren policy.\n",
    "\n",
    "In terms of the Policy, \"CNNPolicy\" - a policy architecture option that is especially good at processing grid-like data (i.e., images) was chosen. This CNN, particularly good for game environments, will take in the image-based observation from the environment, process it through its layers to detect important features, and then output an action (or a distribution over actions) that the agent should take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = PPO(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1,\n",
    "            # Defining the discount factor for future rewards (how much the agent cares about rewards in the distance future relative to immediate ones).\n",
    "            gamma=0.99,  \n",
    "            # Defining the trade-off between exploration (trying out new actions) and exploitation (sticking to what's known to work)\n",
    "            ent_coef=0.01, \n",
    "            # Defining the learning rate for the optimizer.\n",
    "            learning_rate=0.00025, \n",
    "            # Defining the value function coefficient - scaling factor to change the contribution of the value function loss to the total loss function.\n",
    "            vf_coef=0.5, \n",
    "            # Defining the maximum value for the gradient clipping.\n",
    "            max_grad_norm=0.5,\n",
    "            # Defining the trade-off between variance and bias - technique to reduce variance.\n",
    "            gae_lambda=0.95,\n",
    "            # Defining the clipping parameter for the policy update - how much the new policy can deviate from the old policy during each update.\n",
    "            clip_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = PPO(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part IV: Initial Training**\n",
    "\n",
    "Initially train the model for a small number of steps to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the short model training path.\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_15_Driving_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the short model timesteps - 15.\n",
    "#model.learn(total_timesteps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model.\n",
    "#model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model.\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to see if it was properly saved.\n",
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part V: Initial Evaluation**\n",
    "\n",
    "Evaluating the performance of the agent after being trained with the short timestepped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the model.\n",
    "evaluate_policy(model, env, n_eval_episodes=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** Although we can see some improvements in terms of the actions that the car takes, the agent is definetly not trained as most of its time is spent on grass instead of trying to complete the track. Given such poor generated scores, the hypothesis that can be raised is that with a larger amount of training steps, the agent will be able to learn a better policy in order to take actions that will maximize the accumulated score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VI: Training Stage**\n",
    "\n",
    "Now that the training process is understood and after confirming the poor results from only training the agent on 15 timesteps, the model will be trained for a longer period of time to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the long model training path.\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_100_Driving_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the long model on 100K timesteps.\n",
    "#model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model.\n",
    "#model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model.\n",
    "#del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VII: Testing and Evaluation Stages**\n",
    "\n",
    "Evaluating the performance of the testing performed with the long timestepped model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Define the Environment:**\n",
    "Defining the type of environment - 'CarRacing-v2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the type of environment.\n",
    "environment_name = 'CarRacing-v2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of it and setting the render mode as \"human\" so the environment can be seen on video.\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "# Wraping the nevironment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Load the Model:**\n",
    "\n",
    "Loading the longer trained model from its orginal path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_100_Driving_model')\n",
    "model = PPO.load(ppo_path, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Evaluation:**\n",
    "\n",
    "This step is done to measure the performance of your trained agent. Evaluation gives us an estimate of how well the agent will perform and can be used to compare different models or configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the longer trained model.\n",
    "evalue = evaluate_policy(model, env, n_eval_episodes=2, render=True)\n",
    "print(evalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Testing:**\n",
    "\n",
    "This step is done as a final assessment of the agent, in order to check how it would operate in the real world given the model that it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running multiple episodes and record the scores and time steps.\n",
    "episodes = 3\n",
    "scores_array = []\n",
    "timestep_arr = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    # Resetting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0\n",
    "\n",
    "    # Initializing the timestep as 0.\n",
    "    timestep = 0\n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "\n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Incrementing the timesteps.\n",
    "        timestep += 1\n",
    "        \n",
    "        # Render the environment.\n",
    "        env.render()\n",
    "    \n",
    "    # Saving both the scores and timesteps.\n",
    "    scores_array.append(score)\n",
    "    timestep_arr.append(timestep)\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VIII: Score Analysis**\n",
    "\n",
    "Plotting the scores and time steps for each episode to analyze the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the scores and timestep per episode.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_array)+1), scores_array, label=\"Score\", color='red')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,)\n",
    "plt.xticks(np.arange(1, len(scores_array)+1))\n",
    "plt.xlabel('Episodes #')\n",
    "plt.title(\"Scores over Episodes\", fontweight='bold' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As observable, with the longer trainedstepped model the results far exceeded the ones from the first testing stages. Despite the better score values, some of irregularity on these still needs to be addressed, hence for now the challenge wasn't fully successful given that none of the runs achieved the necessary 900 points in order to pass it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
