{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ie_logo_png.png\" style= \"width:15%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ambulance_imag.png\" style= \"width:6%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning & Reinforcement Learning\n",
    "# **Group Assignment**\n",
    "### **Highway Environment: Emergency Response Driving Challenge**\n",
    "Professor: David Kremer\n",
    "\n",
    "Assignment done by: \n",
    "- Adrian Marino\n",
    "- Felipe Fischel\n",
    "- Gilles Calderón\n",
    "- Jean-Jacon Klat\n",
    "- João André Pinho\n",
    "- Nicholas Dieke\n",
    "- Niccolo Borgato\n",
    "- Max Heilingbrunner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Resolution**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part 0: Imports & Setup**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Libraries and Module Imports:**\n",
    "\n",
    "Importing the necessary libraries and modules to manipulate and interact with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from highway_env.envs.highway_env import HighwayEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from typing import Dict, Text\n",
    "from highway_env import utils\n",
    "from highway_env.envs.common.abstract import AbstractEnv\n",
    "from highway_env.envs.common.action import Action\n",
    "from highway_env.road.road import Road, RoadNetwork\n",
    "from highway_env.utils import near_split\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "from highway_env.vehicle.kinematics import Vehicle\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import tensorflow as tf\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "import imageio\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import matplotlib.animation as animation\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful pip installs.\n",
    "\n",
    "#!pip install rl-agents\n",
    "\n",
    "#!pip show rl_agents\n",
    "\n",
    "#!pip install highway-env\n",
    "\n",
    "#!pip install pyvirtualdisplay highway-env stable_baselines3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part I: Setting Up and Understanding the Environment and Action Space**\n",
    "\n",
    "Understand the environment - \"Highway-v0\" and action space that will be used and create an instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Setup the Environment:**\n",
    "\n",
    "Defining the environment and instanciating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the type of environment.\n",
    "environment_name = 'highway-v0'\n",
    "\n",
    "# Creating an instance of it and setting the render mode as \"human\" so the environment can be seen on video.\n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Overview of the Environment Configuration:**\n",
    "\n",
    "Holistic view of the current environment configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'type': 'DiscreteMetaAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 40,\n",
      " 'ego_spacing': 2,\n",
      " 'high_speed_reward': 0.4,\n",
      " 'initial_lane_id': None,\n",
      " 'lane_change_reward': 0,\n",
      " 'lanes_count': 4,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'type': 'Kinematics'},\n",
      " 'offroad_terminal': False,\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 1,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'reward_speed_range': [20, 30],\n",
      " 'right_lane_reward': 0.1,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15,\n",
      " 'vehicles_count': 50,\n",
      " 'vehicles_density': 1}\n"
     ]
    }
   ],
   "source": [
    "# Checking the current environment configuration.\n",
    "pprint.pprint(env.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Understand the Current Action Space:**\n",
    "\n",
    "Describing the current shape and type of the environment's action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(5)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Default Action Space.\n",
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** : As of now, the default action space is discrete and uses 5 different actions:\n",
    " \n",
    " - **0: Lane Left**\n",
    " - **1: Idle**\n",
    " - **2: Lane Right**\n",
    " - **3: Faster**\n",
    " - **4: Slower**\n",
    "\n",
    " This means the action space consists of 5 discrete actions, each of which can be represented as an integer from 0 to 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Understand the Current Observation Space:**\n",
    "\n",
    "Describing the current shape and type of the environment's observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (5, 5), float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Default Observation Space.\n",
    "env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The default Observation Space consists of a 2D 5x5 matrix, where each row corresponds to one vehicle in this environment and the columns correspond to different characteristics of that vehicle. For these observations, each element is a real number (a float), and there is no specific limit on the values that these numbers can take. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Change the Observation Space:**\n",
    "\n",
    "Changing the observation space from array-based to image-based based on the provided documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Changing the Observation Space to a Grayscale Image based one.\n",
    "%matplotlib inline\n",
    "config = {\n",
    "   \"observation\": {\n",
    "       \"type\": \"GrayscaleObservation\",\n",
    "       \"observation_shape\": (128, 64),\n",
    "       \"stack_size\": 4,\n",
    "       \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "       \"scaling\": 1.75,\n",
    "   },\n",
    "   \"policy_frequency\": 2\n",
    "}\n",
    "\n",
    "env.configure(config)\n",
    "obs, info = env.reset()\n",
    "\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "   ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of the staking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9EAAACVCAYAAABfEXmUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4tUlEQVR4nO3deXRUVZ4H8G8qG4GQBBJICISwJSSAIFJkIWGJZFiURcAR6NBHPWrbjjpHmfGo44LtaRpP9zj02OOoPd0taiMSVERUsAGzAUmUEBYhiQkkJGRfqyp7LXf+4NQzEah3KySpqvj9nFPnaOX3Xm4e9/veve+9euUmhBAgIiIiIiIiIlUaRzeAiIiIiIiIyFVwEk1EREREREQkiZNoIiIiIiIiIkmcRBMRERERERFJ4iSaiIiIiIiISBIn0URERERERESSOIkmIiIiIiIiksRJNBEREREREZEkTqKJiIiIiIiIJHESTURERERERCRpwCbRb775JiZNmoRhw4YhNjYW33777UD9KiIaIMwx0dDALBO5PuaYyHkMyCR679692Lp1K7Zt24bTp09jzpw5WL58Oerq6gbi1xHRAGCOiYYGZpnI9THHRM7FTQgh+nulsbGxmD9/Pv7nf/4HAGCxWBAWFoYnn3wSzz33nM1lLRYLqqqqMHLkSLi5ufV304iGBCEEDAYDQkNDodEMzA0lt5Jjaz2zTGSbs2eZOSaSM9BZ5jGZaODZk2OP/v7l3d3dyMvLw/PPP6+8p9FokJycjOzs7Ovqu7q60NXVpfx/ZWUlZsyY0d/NIhqSKioqMGHChH5fr705BphlolvhLFlmjoluzUBkmcdkosElk+N+n0Q3NDTAbDYjODi41/vBwcEoLCy8rn7Hjh34zW9+c937s2fPhru7e383j2hIMJvNOHfuHEaOHDkg67c3xwCzTNQXzpZl5piobwYyyzwmEw0Oe3Lc75Noez3//PPYunWr8v96vR5hYWFwd3dnyIlUONMtWcwyUd85S5aZY6JbwywTuT6ZHPf7JDooKAju7u6ora3t9X5tbS1CQkKuq/f29oa3t3d/N4OIboG9OQaYZSJnxGMykevjMZnI+fT7kw+8vLwwb948HDt2THnPYrHg2LFjiI+P7+9fR0QDgDkmGhqYZSLXxxwTOZ8BuZ1769atuP/++6HVahETE4M//vGPaGtrw4MPPjgQv46IBgBzTDQ0MMtEro85JnIuAzKJ3rhxI+rr6/Hyyy+jpqYGt99+Ow4fPnzdAxGIyHkxx0RDA7NM5PqYYyLnMiDfE30r9Ho9/P39MXfuXD74gOgmzGYz8vPzodPp4Ofn5+jm3BCzTKTO2bPMHBPJYZaJXJ89Oe7/b4MnIiIiIiIiGqI4iSYiIiIiIiKSxEk0ERERERERkSROoomIiIiIiIgkcRJNREREREREJImTaCIiIiIiIiJJnEQTERERERERSeIkmoiIiIiIiEgSJ9FEREREREREkjiJJiIiIiIiIpLESTQRERERERGRJE6iiYiIiIiIiCRxEk1EREREREQkycPRDbiZ3/72txgxYoSjm0HklNra2nD33Xc7uhlSmGWim3OVLDPHRLYxy0Suz54cO+0ketKkSRg5cqSjm0HklAwGg6ObII1ZJro5V8kyc0xkG7NM5PrsybHTTqIfeugheHg4bfOIHMpkMjm6CdKYZaKbc5UsM8dEtjHLRK7Pnhw7bYq6urpcZodEvbW2tkKn09m1zOrVqzF69Gh88803sFgsA9SyocNsNju6CdKYZdfS0tKCtrY26XoPDw9s2rQJnZ2dyM7OHsCWDU2ukmXm2Hl1dHSgqanJrmWWLVuGsLAwHD16lP+u/YRZJnJ99uTYaSfR5Lo6OjpQXV0NT09PuLm52ay1WCwwmUyYNWsWJk+ejPT0dE6iiRxIr9ejrq4OXl5eqrVmsxnu7u5YsGABWlpaOIkmcgDrMdfDwwMaje3nxVqPuZGRkYiNjUV6ejonVEREfcBJNA0INzc3PPzwwwgNDbVZV1hYiN27d+P9999HYGAgD+ZETmD48OH413/9V9WHz2RmZuLYsWP44x//yNsDiRzsl7/8JaZMmWKzpqysDH/961+xb98+ZGVlobOzc5BaR0Q0tHDUQwOmpqZG9apyQ0MDAGDMmDEIDg5GY2PjYDSNiGywWCyoqqqCj4+PzTqdTgc3NzeEhoZCCGH3xziIqP/U19er3kFSV1cHAAgMDMS4cePQ3NzsMrchE7kyvV5/06y5ubnBz89P9U4Sci6cRNOAEEJg//790vVr1qzB1KlT8fLLL/NqNJGDdXZ24v3335eq9fb2xpYtW9DU1IT//u//HuCWEdHNfPHFF9K1y5cvR1xcHP7jP/6Dk2iiASaEQEVFBTo6Om74c41GgxkzZmDYsGGD3DK6FZxEU7/z9/fHtGnT7Frm66+/ho+PDyfQRA42ZswY+Pn5Sde7u7vj73//O4xG4wC2iohuZuTIkXYfczMzM3H+/Hl0d3cPUKuI6KcmTpyItWvXXve+yWTC559/jtbWVuW9sLAwPP/880hLS8OZM2cGsZUki5No6ndeXl5SDyXq6erVqwPUGiKyh4+Pj+pt3D9VXFw8QK0hIjWenp4ICAiwa5mamhrU1NQMTIOI6IY8PT0xevTo6943mUzw9fVVPgLZ2NiIjo4OjB49mlennRgn0URERERERAPo8uXL2L59+3Xve3l54bnnnkNgYCAA4I033kBFRQVeeeUVCCEGu5kkyWkn0Y8++qhyNaS8vBwHDhzAihUrEBERYXO5xsZGfPTRR0hISMDtt98u9btOnz6NnJwcbNq06YZniHoqLCzEkSNHAADBwcH453/+Z9WvcTp06BDq6+vxi1/8QvUJtpmZmTh79iwAYNasWUhKSrJZbzabsWfPHgQEBGDVqlU2a61qa2uRmpqKpUuXYsaMGTZrdTodPvzwQ8ybNw8xMTFS6//++++RlpaGjRs3YuzYsTZrS0tLlc9xjR49Gps2bYK7u7vNZY4dO4YrV64gJSUF3t7eNmtzcnLw3XffAQAiIiKwYsUKm/VCCOzduxfe3t5Yt26dzVqrxsZG7N27FwsWLFDtc21tbdi9eze6urrg7u5uV59bv349xo8fD+DaV5r8+te/lmqfo/XMck96vR67d+/G3LlzERcXZ3MdXV1d+Pvf/4729nYAwH333Yfg4GCby5SVleHgwYNYtWoVJk+eLNXWtLQ0XL58GSkpKapnf7/99lvk5eUhJSVF6vZnIQT27dsHDw8PrF+/XrX+4MGDKCsrAwAkJCTgjjvusFnf3t6O3bt3IyoqCgsXLrRZazQasXv3bhgMBgDA2rVrMXHiRJvLVFZW4tNPP8U//dM/ISoqSrX9AHD8+HFcvHgRKSkpqk/6PnPmDLKysgAA4eHhWLNmjer69+/fj66uLmzcuFF1P3z48GHlivn8+fOl+tzu3bsRHh6OpUuXqrYF+LHP3X333apPSa6rq8PevXuxcOFC5OfnS63fkW6W455MJhM+/PBD5QFzMv2qqqoKn3zyiVS/am5uxp49e2A2m+Hl5YWUlBT4+vraXObMmTM4ceIENm7ciKCgIJu1JSUlOHToEAAgKCgIGzduVH3Qzz/+8Q9UVlYiJSVF6g6s7u5u7N69G2FhYUhOTrZZa7FY8NFHHykP3LzrrrswdepUm8vU19cjNTUVCxcuxOzZs23W6vV67NmzB93d3fDw8MDmzZtVr2gXFBTg6NGjuPfeezFu3DibtVZfffUVGhsbsXnzZtUxUHp6OoqLi7FlyxapO2JMJhP27NmDwMBA3HXXXTZrhRD4+OOPlSvwycnJiI6OtrlMS0sL9uzZg/nz50Or1dqsdZXj8qOPPgpvb2+kpqZi2LBhuOeee1SX+eyzz1BRUQEAWLhwoepYp7W1Fbt378Ztt92GBQsWSLXLOtbZsGGD6je7WOcFwLWPEMqOr4uKipCSkoLhw4fbrM3Ly8PJkycBAJMnT5YaX3/yySewWCy49957b3g8amho6PWRRWvfmjdvHubNm4fc3Fzlafn19fUICAjAunXrEBAQADc3N+zevRtTp07FkiVLVNsC/Di+Xr16NSZNmmSztqamBvv27QMAjBgxQnp8nZ+fLzUGOn/+PNLT0wEAEyZMkBpfW29v37Rpk9QD16zj6xkzZiAxMdFmrXUMFBoaimXLlvX6mT05dtpJ9OrVq5V/lNOnT+PAgQOIiYlRnVSWlpYiNTUVc+bMkRqoAtcOarm5uVi2bJnqAf/o0aPKJHr06NFYv3696uDt/PnzaG9vx9q1a1U7ZVVVlTKJnjx5surfYDQacfDgQYwbN0767y0oKMDHH3+MO+64Q/WgU11djb1792LGjBnS6/f09ERGRgbuvPNOREZG2qw9efKkMon28/PDunXrVHeEly5dQm1tLdasWaM6MNfpdMokeuLEiap/g8Viwddffw1fX1/pv7esrAypqamYPXu26jKNjY1ITU1FV1cX3NzckJycrLpzO3bsGI4cOdLrwKXX613iYA30znJPNTU10n2rtbUVH3/8Mdrb26HRaLBkyRLVwU9OTg4OHjyIBQsWID4+XqqtZWVlqKqqwurVqzFy5EibtXq9HmfPnsXy5culBpNCCBw7dgxeXl5SfevUqVPKJHrWrFmqyzQ3NyM1NRXTpk1Tre3s7MSBAweUSfSiRYtUJ+lnzpzBp59+itjYWOlJZV1dHYqLi3HXXXcpZ9hvxmKxKJPosWPHSm2j48ePo7W1FevWrVM9yF68eFGZREdFRamuv62tDZ988gkmTZokvS/Izc3FwYMHER8fj4SEBJu1P/zwA/bt26c60XEWN8txT93d3Thw4IAyiU5MTFSdeJw/fx6ffPIJtFrtdYOZn6qoqEBqairMZjM8PDywcuVKjBkzRrXtJ0+exNKlS1UnoOnp6cok2t/fH+vXr1ftV4WFhWhqasKaNWtUB+bAtZNdn376KcLDw1X7ldlsxqFDh5RJdFxcnOoJspKSEuzbtw+333676uSovr4e+/btQ3d3NzQaDZYtW4YJEybYXObw4cM4evQolixZgpkzZ9qstTpz5gy6urpwzz33qJ5oKC8vR0VFhVR/A671uYMHDyI0NFQqp2lpacoket68eaon1q9evYq9e/dK7YNd5bi8evVq+Pr64h//+Af8/PyktltOTo4yiZYZXzc0NCA1NRWRkZHS+0/r+HrRokWq+8W8vDxlEj1ixAjp8fXly5exatUq1ZNFXV1dyiRadnydnp4Os9ksNS8Aro2vMzMzceedd+Lee+/FX/7yF+WYDwCRkZFISkpCaGgoNBoNPv74Y6l5gZV1fJ2QkIDY2FibtRcvXlROAgwbNkx6fH3+/HmsXLlS9aKGRqNRJtFjxoyR7nMajQbr1q1TvbgG/Di+lulzHR0d2L9/P8LCwq6rtSfHbsLJ7hPQ6/Xw9/fH8uXL4enpCeDaTrKpqQkBAQGqV4dMJhMaGhowcuRI1Q5g1dbWBoPBgKCgINUJXEdHhzJA8PDwUD2zDVw722Q0GhEUFKQaLL1er1xtGz58uOpBRAiBxsZGuLu7Y9SoUaptAa5NvBsbG+Hv7696ptdsNqO+vh6+vr6qZ/ytrNszMDBQ+Te8ma6uLjQ3NwO4tj0DAwNVt1FLSwu6u7sRGBioGqzW1lblQQ3Dhg2T+txYQ0MDNBqN6hViK3v6nHV7WtnT50aPHq0MQIxGI77++mvodDq7HgI1mG6U5Z7s6VtmsxmNjY3K54Xs6VujRo1SPbj2bHNnZ6ddfWvMmDFSO3jAvr7V3NyMrq4uAJDuW42NjRg2bJjUfqOhoUF5Km/PvnUz1v2wzH7DymAwoK2tTWobWfcbwLUnfsvsz5qammCxWKT3w9az/Pb0OS8vL+nPm1r7nMyxyrof9vHxQVZWltNmWS3HPf20X8lkz55+Zd3XWtnTr2T2GZ2dnWhpaQFg3/FI9vgOXOtXTU1NUp9jth7frVev7OlXg3k8UtPS0gKTySS1Pe3ZBwM/biMPDw/p47t1e9rT52S2p7Mfl3+a5YE+HtXX12PEiBGqJ6St7OlbPceO7u7u0uNr2b410McjoPcYaMSIEaiuru51pdrT0xNBQUHKiTzZ47uVPWMg634DuDbh7e8xUM/t6eXlJd3nzGaz1H4DsK/PWY9VN9oP25Njp51Ez507V3pgSvRzYzabkZ+f77QHa4BZJpLh7FlmjonkMMtErs+eHNv1rd47duzA/PnzMXLkSIwdOxb33HMPioqKetUsWbIEbm5uvV6ucHsL0c8Js0zk+phjoqGBWSZyPXZNojMyMvD4448jJycHR44cgdFoxLJly9DW1tar7pFHHkF1dbXy+v3vf9+vjSaiW8MsE7k+5phoaGCWiVyPXQ8WO3z4cK//37VrF8aOHYu8vDwsWrRIeX/48OEICQnpnxYSUb9jlolcH3NMNDQwy0Sux64r0T9lfcDWTz8gvnv3bgQFBWHWrFl4/vnnlQdl3UhXVxf0en2vFxENLmaZyPUxx0RDA7NM5Pz6/BVXFosFTz31FBISEjBr1izl/V/84hcIDw9HaGgozp07h2effRZFRUX49NNPb7ieHTt24De/+U1fm0FEt4hZJnJ9zDHR0MAsE7mGPj+d+7HHHsOhQ4dw/Phxm98r+M0332Dp0qUoKSm54Xc0dnV1KY/NB649PTAsLIxPDySyoT+fAsosEzlOf2WZOSZyLGaZyPXZk+M+XYl+4okn8MUXXyAzM9NmwAEoX/B9s5B7e3tLf4crEfUvZpnI9THHREMDs0zkOuyaRAsh8OSTT2L//v1IT0/H5MmTVZc5c+YMAGDcuHF2NWzlypUYNmwYAKChoQEnT57EvHnzMH78eJvL6fV6ZGZmYsaMGZgyZYrU7yopKUFhYSEWL16s+gXdV69exenTpwEAo0aNQmJiouqXgJ86dQotLS1ISkpSPfv3/fff4/LlywCA8PBwzJkzx2a9xWJBeno6fH19ERMTY7PWqrm5GVlZWbj99tsxceJEm7VtbW1IT09HREQEIiMjpdZfWlqK8+fPY+HChapfUF9dXY3vvvsOADBy5EgsXrxY+WL5m8nPz0d9fT2SkpLg6elps7agoADFxcUAgPHjx2PevHk264UQyMzMhKenJxYsWGCz1spgMCAjIwPR0dE3PJD11NHRgbS0NJhMJmg0Grv6XEJCAgIDAwEAnZ2dyM/Pl2rfjTgqyz21t7cjPT0dU6dOxfTp022uw2g04ptvvlHOqsv0rdraWuTm5iI2NhbBwcFSbT179ixqamqQlJQELy8vm7VFRUUoKSlBUlIShg8frrpuIQSOHz8OjUaDhIQE1frc3FzU1tYCAGbMmIFp06bZrO/q6kJaWhomTJjQ6xbAGzGbzUhLS1M+T7dgwQIEBQXZXKaxsREnTpzAHXfcoTq4s7pw4QKuXLmCpKQk+Pj42Ky9dOkSLly4AAAYM2YM4uPjVdd/8uRJGI1GLFq0SHU/nJeXh8rKSgBAREQEoqOjbdYbjUakpaVhzJgxmDt3rmpbgB/73Pz581VzYt0Pz5gxo89ZdoYc92Q2m5Genq48UTg+Ph5jxoyxuUxTUxOOHz+OuXPnIiwszGZta2sr0tPTYbFY4OHhIdWvLl++jO+//x5LlixRvapQVVWFU6dOAbh2PLJ+pZAtp0+fRkNDA+688054eKgPqUwmE7755hupfiWEQHp6OgwGAwBI9SudToeMjAzcdtttqv2h5/HI3d0dixcvhq+vr81lKioqkJ+fj8TExOs+r3sz3333HfR6PZYsWaI6Bjp37hyqqqpw5513qu6DgR/7nJ+fH+bPn69an5WVhebmZgCQ7nMZGRmIjIxERESEzdpbOS4Pdpa9vb2RmZkJLy8vqX1tdnY26uvrAQCzZs1SHV9b+9akSZMwY8YMqXbdaKxzM/X19cjOzgZw7WFrsuPrq1evIikpSfXkQklJCS5evAgACAkJkRpfnzhxAhaLBQsXLlStBewbX3d3dyMtLQ3jxo3D7NmzpdZvHV/LjIGsxyMAGDZsmPT4urS0FEuWLFEdA1nnBQAQFBQkNb7Ozs5GZ2en1LwA+LHPhYeHY+bMmTZrrfvhoKAg3HHHHb1+ZleOhR0ee+wx4e/vL9LT00V1dbXyam9vF0IIUVJSIl599VVx6tQpUVpaKg4cOCCmTJkiFi1aJP07dDqdACBaWlqExWIRFotF5ObmCq1WK77++mvlvZu9fvjhBxEXFyd2796tWmt97dq1SyxYsEBcunRJtfbLL78UWq1WaLVacf/99wuTyaS6zLPPPivWrl0r2tvbVWv/8Ic/KOvfvn27an1nZ6fYsGGDePrpp6X/3nPnzomYmBixf/9+1dry8nKRmJgo/vznP0uvf9++fSImJkZcuHBBtTY9PV35e++77z7R1dWlusyrr74qVqxYIXQ6nWrtW2+9paz/hRdeUK03Go1iy5Yt4tFHH5X+e4uLi0VcXJz44IMPVGtra2tFUlKS0Gq1Ii4uTpSUlKgu89VXXwmtVitOnTqlvNfS0iIACJ1OZ0+EHZ7lnq+KigqRmJgo3n77bdVt0NLSIpYvXy60Wq2IiYkR586dU10mMzNTaLVakZGRIf1vuX37drFs2TLR3NysWvvOO++IxMREUVFRIbVuk8kk7r//fvHII48Is9msWv/UU08pfXfXrl2q9fX19SIpKUm8/vrrqrVtbW1izZo1yvpzc3NVlzl16pTQarXiq6++kt6eO3fuFElJSaK2tla19oMPPlDa8+STT0qt/9FHHxVbtmwRRqNRtfaFF15Q1v/mm2+q1ut0OrFixQrx6quvSv+9WVlZQqvVirS0NNXaCxcuiJiYGPHee+/1OcvOkOOer/b2dnHPPfco2/nkyZOqy5w+fVpotVpx8OBB1drS0lKxYMECodVqxeLFi0V1dbXqMnv27BFxcXGisLBQtfbIkSNK2zdv3iy6u7tVl3nppZfE3XffLQwGg1Qf0ev1YuXKlWLbtm2qtd3d3WLTpk1Km44ePaq6TEFBgYiNjRV79+5Vra2qqhKLFi0SWq1WJCQkiLKyMtVlPv/8c6HVasWZM2ekc/HMM8+IdevWiY6ODtXa1157TSQnJ4umpiapdXd0dIh169aJZ555RrXWbDaLBx98UNmen3/+ueoyZWVlIiEhQfztb39Trb2V4/JgZ9loNIqUlBTx2GOPSW3nJ554QtluMuPrmpoasWTJEvHGG29I9xPr+DovL0+1NicnR2mPPePrpUuXioaGBtXad999V1m/zPjabDaLhx9+WDzwwANS8wKLxb7xdVNTk1i2bJnYsWOH9Pa0jq+zsrJUa8+ePStiYmKEVqu1a3y9aNEiUVlZqVqbmpqqbE/Z8fWTTz4pNm7cKDUvsFh+HF/v3LlTtdZgMIhVq1aJF1988bqf2ZNjuz4TfbMzsu+++y4eeOABVFRUYMuWLfj+++/R1taGsLAwrFu3Di+++KL050P0ej38/f3xy1/+UjkL2draikuXLiE8PBwBAQE2l+/s7ERRURHGjx+vemXFqq6uDtXV1YiKilI9O9XU1ISKigoA185+qZ2ZBIArV66gvb0d06dPVz2bUllZiYaGBgBAYGCg6hUfi8WCH374AV5eXtJX3tvb21FcXIywsDDVM8nd3d0oKirC2LFjpa/mNTY24urVq4iIiFA9O6XX61FaWgrg2tmviIgI1W1UUVEBvV6PqKgo1TOPNTU1ytW8UaNGqV55B65dYfTw8FC9qmxl7XOhoaGqV12MRiOKiopgNpsBANOnT1e9utPS0oIrV65g2rRpGDFiBIBr/y4ffPBBnz975ags92TtW2PGjFH9yg6z2YzCwkKYTCYAsKtvTZ48WbrNV69ehU6nk+pbtbW1qKurw/Tp06WumABAcXExNBqNVN+6fPmycgVq3LhxGDt2rM16k8mEwsJCjB49GqGhoTZrLRYLioqK0N3dDQCYOnWq6hWotrY2lJSUSO2HraqqqtDU1ITp06erntWur69HVVUVgGtXAWX2Z5cuXYLJZFK9kwEAysvLlStQwcHB0n3Oz89P9WqVlcFgwOXLlzFp0iT4+/vbrLXuh4ODg/Hll1/2KcvOkOOerMcj6x0jU6ZMUb3TxtqvZI5HXV1dKCwsBAC4u7tL9auGhgZUVlZK7TN0Oh3KysoAyB+PysvLYTAYEB0dLXW1xGKxoKCgQKpfWSwWFBcXo7OzEwDs6lcyYyCj0YjCwkJYLBZoNBpERkZKj4FktqdVWVkZOjs7ERkZKTUGamlpwfTp06Wu7Fv73LBhwzBp0iTV+pKSEuVOCdk+98MPPyA4OFh1H3wrx2VHZLmoqAienp5S+9qexyPZvlVUVCR1PLKy9q2eY52bsc4LAMDLy0t6fC3bt6zzAkD+eFRSUgIhhNS8ALBvfG09Hvn7+0vfCWYdA8nsh637DQDw8PCQHl/X19dLjYGs8wIA8PX1lR4DGY1Gqf0wYF+fs46BRowYcd28wJ4c9/nBYgPFGvKh9OCDzs5OZfAvw93dHePHj0d3dzdaW1sHsGXUn7q7u5VJiQyNRoOIiAhYLBZlZy2rPx8sNlCYZWbZldj7b+vl5YXIyEgYDAY0Njb2+fc6e5aZY+bYWRmNxl4PzpJhHZBbP9rRn5jlwccsU38b8AeLkX0qKyuVKyAyRowYgU2bNqG2thY5OTkD2DLqT7W1tcpVbxmenp743e9+h7a2NuzcuXMAW0b9hVkeuioqKpTvZpUxduxYvPPOOzhx4gQ++uijAWwZ9TfmeGiw3mUgS6PR4KWXXsLw4cOxffv2AWwZDRZmmRyJk+hB4u3tjTlz5qjeklBWVoampiacPn3arqua5Bw8PT2xatUq1dvcCgoKcObMGRw4cGCQWkb9hVkemoQQ8Pf3x1133aX6b5udnY26ujrs2bMHdXV1g9RC6k/M8dDg7u6Ou+++W/V21UuXLiEnJwdfffWV6q3C5FqYZXIUTqIHiYeHB8LCwlQ/h9HQ0ICGhgZUVVWpPhWUnI+Hhwduu+021VtAdDod8vPzcfHiRX4FhYthlocuHx8fzJkzR/XftqioCJWVlThz5gwsFssgtY76E3M8NGg0GkRHR0s9jyQnJweFhYWqz38g18IsOzeTyYSbfXLYzc1N6tkHzsp1W+5i2tvb8cUXX6jWmUwmeHt7Y/ny5airq1O+boNcQ0dHB15//XXVHXR3dzc8PDzw1FNPob29HW+99dYgtZBuFbM8dNXV1eG3v/2tal1nZydGjRqFl19+Gd9++y0+/fTTQWgd9SfmeGgwGo3405/+pHoV0mg0ws3NDY899hhGjBiB119/fZBaSAONWXZe1gee3uxz615eXoiKipJ6cJgz4iR6EPj5+dl1psXb2xtVVVV2fT6PHM/X11f6qpSPjw/c3d1x8eJF3lbkQpjloSsgIED6rhAfHx/4+fkhPz8f5eXlA9wy6m/M8dDg4+OjegW6Z62bmxuKi4ulv02BnB+z7PxMJhPGjh17w6eWm0wmVFZW9ho7h4WFITExEWfOnLmlh3YOBk6iB4HsTr6nM2fO9H9DaECNGjUKo0aNsmuZQ4cODVBraCAwy0OX2tfX3MjevXsHoCU00JjjoSEgIED66/asjh07NjCNIYdgll1DREQENm3adN37DQ0N+N3vfqc8Zd9isSAiIgJbtmxBXV0dJ9FERERERET08+Hu7o7IyEhUVlbitddeu+7nvr6+eOKJJ+Dh4QGTyYS//OUvKCwsxKuvvmr3V786gtNOomfPnq3ccmMwGFBSUoJJkyapXunr7OxEQUEBJkyYIH2Gqq6uDpWVlYiOjsawYcNs1jY3N6OsrAwAMHz4cEyfPl11/aWlpejo6JC67//q1auor68HAAQFBSEsLMxmvRAChYWF8Pb2lvoyeODa50eKiooQFhaGoKAgm7Xd3d0oKChAcHAwQkJCpNbf0NCAiooKTJ8+XfUp1TqdDpcvXwYADBs2DFFRUaqfJy4vL4dOp0N0dLTqbTzV1dWoqakBcO1K8aRJk1TbX1RUBHd3d0ybNk21Fvixz40fP171apbRaERBQQHMZjMA2NXnIiIilAeidHd3Iz8/X6p9jtYzyz1Z+9bYsWMxbtw4m+swmUwoKChQPlcj07f0ej0uXbqEKVOmwN/fX6qtFRUVaGlpkepbNTU1qK2tRXR0tPTtgUVFRcr3g6u5fPmycsuZTN+ybqNRo0ZhwoQJNmstFgsKCgqUjxJMmzZN9em2ra2tKC4ultoPW129ehVNTU2Ijo6Gp6enzVrrfhgARo4cKZW/kpISmM1mqf1wWVmZ8lUoISEh0n3O398fEydOVF0/YF+fs+6Hx48f7xJZvlmOexJCoKCgQLmqYE+/Cg8Px+jRo23WdnV14eLFiwCuDc5k+lV9fT2uXr2KqKgo+Pj42KxtaWlBaWkpgGvHo+joaJv1AHDlyhW0trYiOjpa6nN9FosFFy5cgJ+fH8LDw23WWo/vnZ2dAIDJkyerXnnt6OhAYWGh1PG95/FIo9EgKipK9SMNTU1NuHLlSq/jkRrrGCg6Olr1+H716lU0NzdL7YOBH/ucj48PJk+erFr/ww8/oK2tDQCk+1xhYSFCQkIQHBxss9ZVjsvWLNsz1ikpKYHBYAAAqfG1tW8FBQUhNDRUql03GuvcjHVeAFy7RVt2fC3bt2pra1FVVQUA8Pf3lxpfFxcXQwiByMhI1VrAvvG19XgUEBCgOi+wso6vp06dqvqwW+t+GLj2oDbZ8XVdXZ3UGKi4uFj5DPrIkSOVMZCXl5fy4DHr+E6j0cDLywvu7u7S8wLgxz43evRo6TGQr6/vdfthu3IsnIxOpxMAREtLi7BYLMJisYjc3Fyh1WrF119/rbx3s9cPP/wg4uLixIcffqhaa3299957YsGCBeLSpUuqtV9++aXQarVCq9WK+++/X5hMJtVlnn32WbF27VrR3t6uWvuf//mfyvq3b9+uWt/Z2Sk2bNggnn76aem/99y5cyImJkbs379ftba8vFwkJiaKP//5z9Lr//jjj0VMTIy4cOGCam16erry9953332iq6tLdZlXX31VrFixQuh0OtXat956S1n/Cy+8oFpvMpnEli1bxKOPPir99xYXF4u4uDjxwQcfqNbW1taKpKQkodVqRVxcnCgpKVFd5tChQ0Kr1YpTp04p77W0tAgAQqfTOTqyN3WjLPd8VVRUiMTERPH222+rbgOdTieWL18utFqtiImJEefOnVNdJjMzU2i1WpGRkSH9b7l9+3axbNky0dzcrFr7zjvviMTERFFRUSG1bpPJJO6//37xyCOPCLPZrFr/1FNPKX33vffeU62vr68XSUlJ4vXXX1etbWtrE2vWrFHWn5ubq7rMqVOnhFarFYcOHZLenjt37hRJSUmitrZWtfaDDz5Q2vPkk09Krf/RRx8VW7ZsEUajUbX2hRdeUNb/5ptvSvW5FStWiFdffVX6783KyhJarVakpaWp1l64cEHExMSI999/36mzrJbjnq+Ojg5xzz33KNs5OztbdZnTp08LrVYrDh48qFpbWloqFixYILRarVi8eLGorq5WXWbPnj0iLi5OFBYWqtYeOXJEafvmzZul+tVLL70k7r77bmEwGKT6iF6vFytXrhTbtm1Tre3u7habNm1S2nT06FHVZQoKCkRsbKzYu3evam11dbVYtGiR0Gq1IiEhQZSVlaku8/nnnwutVivOnDkjnYtnnnlGrFu3TnR0dKjWvvbaayI5OVk0NTVJrbujo0OsW7dOPPPMM6q1ZrNZPPjgg8r2/Pzzz1WXKSsrEwkJCeJvf/ubaq2zH5d7ZtloNIqUlBTx2GOPSW3nJ554QtluMuPrmpoasWTJEvHGG29I9xPr+DovL0+1NicnR2mPPePrpUuXioaGBtXad999V1m/zPjabDaLhx9+WDzwwANS8wKLxb7xdXNzs1i2bJnYsWOH9Pa0jq+zsrJUa8+ePStiYmKEVqu1a3y9aNEiUVlZqVqbmpqqbM+e4+vLly+LkSNHCgDKKzk5WVy4cEE8+OCDYuPGjaK7u1vq77WOr3fu3KlaazAYxKpVq8SLL7543c/sybGbEDd57riD6PV6+Pv747nnnlOu0DU0NCAnJwd33HGH6hktnU6HEydOIDo6WuqsJHDt+wOLioqwcOFC1bPmV69eVT5PERAQgISEBNUzJHl5edDpdFi8eDHc3d1t1l64cEE5Ex4eHo7bbrvNZr3FYkFmZiZGjBiB+fPn26y1am5uxokTJ3Dbbbepnglva2tDZmYmpkyZInW1B7h2Zv78+fNITExUPWteU1PT6+zUwoULVc8mnj17FnV1dViyZInqVYiioiLl7FpoaCjuuOMOm/VCCBw/fhweHh6Ij4+3WWtlMBiQlZWFqKgo1bOVnZ2dSE9Ph8lkgkajkepzlZWVyM/PR3x8PAIDA5X1vPbaa9DpdKpnGB3lRlnuqb29HRkZGVJ9y2g0Ij09XbnClZCQoHo1tLa2Ft999x3mz5+vegXB6vz586ipqcGiRYtUr8gUFRXh8uXLWLx4sepVceBa3zpx4gQ0Gg3i4+NV9xvfffcdamtrAVy7Y2Hq1Kk267u6upCWloawsDDMnDnTZq3JZEJmZiba29sBAHFxcapXrRobG5GdnY25c+di/PjxNmutLl68iPLycixZskT1jovLly8rVxnHjBmD2NhY1fVnZ2fDZDIhMTFRdXuePn1aubIQEREh3efGjh2LOXPmqLYF+LHPabVa1SsLLS0tOH78OKZPn47du3c7bZbVctyT2WxGRkaG0q9iY2NVr1g1NTXh5MmTuP3221WvHrS2tiIzMxMWiwUeHh5YvHix6tXl0tJSXLhwAQsXLlS9O6C6uhp5eXkA5I9H+fn5aGxsxJIlS6SunBqNRmRkZCAoKAi33367zVqLxYKsrCzlCuC8efNU76DQ6XTIysrCrFmzVO+86ujoQEZGBkwmE9zd3bFw4ULVK4AVFRU4e/as1D7Y6tSpUzAYDFi0aJHqGOj8+fOorq7G4sWLpR70ZzabkZmZiZEjR0Kr1dqsFULg5MmTyh0pc+bMUb2q19raiqysLERERKhesXX243LPLHt7e+P48ePw9PREXFyc6rK5ubnKXZIzZ85UHV9bxzrh4eFSd3QAP46ve451bqa+vh65ubkArt0VKju+rqyslOpbJSUlKCwsBAAEBwerjq+FEMjOzobFYpGaFwD2ja+7urqQmZmJkJAQ1XmBlXV8LTMGss4LgGtX9mXH16WlpVi0aJHqGKisrAzff/89ACAwMFAZX7e2tuKzzz7r9fTu0NBQxMTE4MKFCzCZTFL7YeDHPjdx4kTMmDHDZq3JZEJ6ejoCAwMxd+7c69Yjm2OnnUTPnTtXNRBEP1dmsxn5+flOe7AGmGUiGc6eZeaYSA6zTOT67Mmxa34xFxEREREREZEDcBJNREREREREJImTaCIiIiIiIiJJnEQTERERERERSeIkmoiIiIiIiEgSJ9FEREREREREkjiJJiIiIiIiIpLk4egG3ExISIjqF30T/VwZjUZHN0Eas0x0c66SZeaYyDZmmcj12ZNjNyGEGMC22M36ZfDV1dVO+WX1RM5Ar9dj3LhxUl8G7yjMMpE6Z88yc0wkh1kmcn325Nhpr0Tv378fPj4+jm4GkVPq6OhwdBOkMctEN+cqWWaOiWxjlolcnz05dtpJ9P/93//B3d3d0c0gckpms9nRTZDGLBPdnKtkmTkmso1ZJnJ99uSYDxYjIiIiIiIiksRJNBEREREREZEkTqKJiIiIiIiIJHESTURERERERCSJk2giIiIiIiIiSZxEExEREREREUniJJqIiIiIiIhIkl2T6FdeeQVubm69XlFRUcrPOzs78fjjjyMwMBC+vr7YsGEDamtr+73RRHRrmGUi18ccEw0NzDKR67H7SvTMmTNRXV2tvI4fP6787Omnn8bBgwexb98+ZGRkoKqqCuvXr+/XBhNR/2CWiVwfc0w0NDDLRK7Fw+4FPDwQEhJy3fs6nQ5//etf8eGHH+LOO+8EALz77ruIjo5GTk4O4uLibr21RNRvmGUi18ccEw0NzDKRa7H7SnRxcTFCQ0MxZcoUpKSkoLy8HACQl5cHo9GI5ORkpTYqKgoTJ05Ednb2TdfX1dUFvV7f60VEA49ZJnJ9zDHR0MAsE7kWuybRsbGx2LVrFw4fPoy33noLpaWlWLhwIQwGA2pqauDl5YWAgIBeywQHB6Ompuam69yxYwf8/f2VV1hYWJ/+ECKSxywTuT7mmGhoYJaJXI9dt3OvXLlS+e/Zs2cjNjYW4eHhSE1NhY+PT58a8Pzzz2Pr1q3K/+v1egadaIAxy0SujzkmGhqYZSLXc0tfcRUQEIDIyEiUlJQgJCQE3d3daGlp6VVTW1t7w894WHl7e8PPz6/Xi4gGF7NM5PqYY6KhgVkmcn63NIlubW3FpUuXMG7cOMybNw+enp44duyY8vOioiKUl5cjPj7+lhtKRAOHWSZyfcwx0dDALBM5P7tu5/73f/93rF69GuHh4aiqqsK2bdvg7u6OzZs3w9/fHw899BC2bt2K0aNHw8/PD08++STi4+P55EAiJ8MsE7k+5phoaGCWiVyPXZPoq1evYvPmzWhsbMSYMWOQmJiInJwcjBkzBgCwc+dOaDQabNiwAV1dXVi+fDn+93//d0AaTkR9xywTuT7mmGhoYJaJXI+bEEI4uhE96fV6+Pv7Y+7cuXB3d3d0c4icktlsRn5+PnQ6ndN+zolZJlLn7FlmjonkMMtErs+eHN/SZ6KJiIiIiIiIfk44iSYiIiIiIiKSxEk0ERERERERkSROoomIiIiIiIgkcRJNREREREREJImTaCIiIiIiIiJJnEQTERERERERSeIkmoiIiIiIiEgSJ9FEREREREREkjiJJiIiIiIiIpLESTQRERERERGRJE6iiYiIiIiIiCRxEk1EREREREQkycPRDfgpIQQAwGw2O7glRM7Lmg9rXpwRs0ykztmzzBwTyWGWiVyfPTl2ukm0wWAAAJw7d87BLSFyfgaDAf7+/o5uxg0xy0TynDXLzDGRfZhlItcnk2M34WSnzCwWC4qKijBjxgxUVFTAz8/P0U1yKXq9HmFhYdx2feBK204IAYPBgNDQUGg0zvmpDGb51rhSf3Q2rrTtnD3LzPGtcaW+6Gxcbdsxy0Obq/VHZ+JK286eHDvdlWiNRoPx48cDAPz8/Jx+Yzsrbru+c5Vt54xnuntilvsHt13fucq2c+YsM8f9g9uu71xp2zHLQx+3Xd+5yraTzbHznSojIiIiIiIiclKcRBMRERERERFJcspJtLe3N7Zt2wZvb29HN8XlcNv1Hbdd/+M27Ttuu77jtutf3J59x23Xd9x2/Y/btO+47fpuqG47p3uwGBEREREREZGzcsor0URERERERETOiJNoIiIiIiIiIkmcRBMRERERERFJ4iSaiIiIiIiISBIn0URERERERESSnHIS/eabb2LSpEkYNmwYYmNj8e233zq6SU7llVdegZubW69XVFSU8vPOzk48/vjjCAwMhK+vLzZs2IDa2loHtthxMjMzsXr1aoSGhsLNzQ2fffZZr58LIfDyyy9j3Lhx8PHxQXJyMoqLi3vVNDU1ISUlBX5+fggICMBDDz2E1tbWQfwrXBNzrI5ZlscsOw6zrI5ZlscsOwZzrI45lsccO+Ekeu/evdi6dSu2bduG06dPY86cOVi+fDnq6uoc3TSnMnPmTFRXVyuv48ePKz97+umncfDgQezbtw8ZGRmoqqrC+vXrHdhax2lra8OcOXPw5ptv3vDnv//97/HGG2/g7bffRm5uLkaMGIHly5ejs7NTqUlJScGFCxdw5MgRfPHFF8jMzMSvfvWrwfoTXBJzLI9ZlsMsOwazLI9ZlsMsDz7mWB5zLIc5BiCcTExMjHj88ceV/zebzSI0NFTs2LHDga1yLtu2bRNz5sy54c9aWlqEp6en2Ldvn/JeQUGBACCys7MHqYXOCYDYv3+/8v8Wi0WEhISIP/zhD8p7LS0twtvbW+zZs0cIIcTFixcFAPHdd98pNYcOHRJubm6isrJy0NruaphjOcxy3zDLg4dZlsMs9w2zPDiYYznMcd/8XHPsVFeiu7u7kZeXh+TkZOU9jUaD5ORkZGdnO7Blzqe4uBihoaGYMmUKUlJSUF5eDgDIy8uD0WjstQ2joqIwceJEbsOfKC0tRU1NTa9t5e/vj9jYWGVbZWdnIyAgAFqtVqlJTk6GRqNBbm7uoLfZFTDH9mGWbx2zPDCYZfswy7eOWe5/zLF9mONb93PJsVNNohsaGmA2mxEcHNzr/eDgYNTU1DioVc4nNjYWu3btwuHDh/HWW2+htLQUCxcuhMFgQE1NDby8vBAQENBrGW7D61m3h63+VlNTg7Fjx/b6uYeHB0aPHs3teRPMsTxmuX8wywODWZbHLPcPZrn/McfymOP+8XPJsYejG0D2W7lypfLfs2fPRmxsLMLDw5GamgofHx8HtoyI7MEsEw0NzDKR62OOyR5OdSU6KCgI7u7u1z3prra2FiEhIQ5qlfMLCAhAZGQkSkpKEBISgu7ubrS0tPSq4Ta8nnV72OpvISEh1z14w2QyoampidvzJpjjvmOW+4ZZHhjMct8xy33DLPc/5rjvmOO++bnk2Kkm0V5eXpg3bx6OHTumvGexWHDs2DHEx8c7sGXOrbW1FZcuXcK4ceMwb948eHp69tqGRUVFKC8v5zb8icmTJyMkJKTXttLr9cjNzVW2VXx8PFpaWpCXl6fUfPPNN7BYLIiNjR30NrsC5rjvmOW+YZYHBrPcd8xy3zDL/Y857jvmuG9+Njl29JPNfuqjjz4S3t7eYteuXeLixYviV7/6lQgICBA1NTWObprT+Ld/+zeRnp4uSktLxYkTJ0RycrIICgoSdXV1Qgghfv3rX4uJEyeKb775Rpw6dUrEx8eL+Ph4B7faMQwGg8jPzxf5+fkCgPiv//ovkZ+fL65cuSKEEOK1114TAQEB4sCBA+LcuXNi7dq1YvLkyaKjo0NZx4oVK8TcuXNFbm6uOH78uIiIiBCbN2921J/kEphjOcyyPGbZMZhlOcyyPGZ58DHHcphjecyxEE43iRZCiD/96U9i4sSJwsvLS8TExIicnBxHN8mpbNy4UYwbN054eXmJ8ePHi40bN4qSkhLl5x0dHeJf/uVfxKhRo8Tw4cPFunXrRHV1tQNb7DhpaWkCwHWv+++/Xwhx7TH8L730kggODhbe3t5i6dKloqioqNc6GhsbxebNm4Wvr6/w8/MTDz74oDAYDA74a1wLc6yOWZbHLDsOs6yOWZbHLDsGc6yOOZbHHAvhJoQQg3fdm4iIiIiIiMh1OdVnoomIiIiIiIicGSfRRERERERERJI4iSYiIiIiIiKSxEk0ERERERERkSROoomIiIiIiIgkcRJNREREREREJImTaCIiIiIiIiJJnEQTERERERERSeIkmoiIiIiIiEgSJ9FEREREREREkjiJJiIiIiIiIpL0/30ZZW6R+wH4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustrating the stacking.\n",
    "for _ in range(3):\n",
    "    obs, reward, done, truncated, info = env.step(env.action_type.actions_indexes[\"IDLE\"])\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=4, figsize=(12, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (4, 128, 64), uint8)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the new Observation Space.\n",
    "env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The new Observation Space will consist of a 128x64 Grayscale four image stack from the Gameplay, where each image can take on a value from 0 (indicating black) to 255 (indicating white).\n",
    "\n",
    "This change was performed in order to train a model that would represent more similarity with a real-world scenario. As autonomous driving vehicles primarily rely on visual input from cameras, training an RL agent with similar image-based inputs can make the learned policies more directly tranferable to this setting. Additionally, as images encapsulate a wider array of spacial details, such as positioning of surrounding vehicles, lane markings, and other environmental features, the model may gain a more comprehensive and valuable understanding of the scene, facilitating its learning and precise decion making process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part II: Sampling from the Environment**\n",
    "\n",
    "Perform \"manual\" sampling and let the agent take random actions in the environment in order to give a baseline of what to expect from an untrained agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Render the Environment:**\n",
    "\n",
    "Rendering the environment in a py-game window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering the game environment.\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Input Manual Actions:**\n",
    "\n",
    "Selecting some manual actions to feed the agent in order to describe them, understand their meaning and check the states and final state that are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ended. The Info dictionary at the terminal state was: {'speed': 25.746634347414496, 'crashed': True, 'action': 3, 'rewards': {'collision_reward': 1.0, 'right_lane_reward': 1.0, 'high_speed_reward': 0.5741372092661606, 'on_road_reward': 1.0}}\n",
      "Final Score: 20.506441055881957\n",
      "After move 4 the State was: [[[99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  ...\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]]\n",
      "\n",
      " [[99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  ...\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]]\n",
      "\n",
      " [[99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  ...\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]]\n",
      "\n",
      " [[99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  ...\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]\n",
      "  [99 99 99 ... 99 99 99]]]\n"
     ]
    }
   ],
   "source": [
    "# Manually input actions and understand what each action means:\n",
    "def execute_predefined_actions(env):\n",
    "    \n",
    "    # Defining a sequence of actions.\n",
    "    # (6x Lane Left, Lane Left, Lane Right, Idle, Faster, Faster, Slower, Lane Right)\n",
    "    # Although previousy defined, this sequence might not get fully completed since with the ramdom setting of the environment some of these movements can result in a crash (ending the game earlier).\n",
    "    actions = [0, 2, 1, 3, 4, 0, 2, 1, 3, 4, 0, 2, 1, 3, 4, 0, 2, 1, 3, 4, 0, 2, 1, 3, 4, 0, 2, 1, 3, 4]\n",
    "\n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0 \n",
    "\n",
    "    # Reseting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    state = env.reset()\n",
    "\n",
    "    # Iterating over the provided list of actions.\n",
    "    for action in actions:\n",
    "        \n",
    "        # Continues until the episode is done.\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Addding the reward to the total score.\n",
    "        score += reward\n",
    "\n",
    "        env.render()\n",
    "\n",
    "    # The episode ended. `state` now contains the final state.\n",
    "    if done:\n",
    "        print(\"Episode ended. The Info dictionary at the terminal state was:\", info)\n",
    "        print(\"Final Score:\", score)\n",
    "        print(\"After move\", action, \"the State was:\", n_state)\n",
    "\n",
    "    # Closing the environment.\n",
    "    env.close()\n",
    "\n",
    "# Running the manual sequence of actions.\n",
    "execute_predefined_actions(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:** As expected, the final state configuration instead of being represented by a 2D 5x5 array where each row represents a vehicle (the first one being the ego vehicle - the agent - and the others are the vehicles in the environment):\n",
    "\n",
    "- 1st: **Existance of Active Status in the Environment** - represented by 1.0 in all vehicles;\n",
    "\n",
    "- 2nd: **Longitudinal Position of the Vehicles Along the Road** - Ego vehicle is always at position 1.0, and the other vehicles have a relative position to it;\n",
    "\n",
    "- 3rd: **Lateral Position of the Vehicles** - how far they are to the left or right of the center of the road (positive value indicates a position to the left, a negative value indicates a position to the right);\n",
    "\n",
    "- 4th: **Relative Speed of Vehicles with respect to the Agent** - the speed difference between the agent and the other vehicles;\n",
    "\n",
    "- 5th: **Acceleration of the Vehicles** - the acceleration of the vehicles, which is the change in speed from one time step to the next.\n",
    "\n",
    "It is now represented by a 3-Dimensional stack of 4 grayscale images of the game environment, which shows a constant value of 99 for the pixels because these are the terminal frames when the game stopped due to the crash.\n",
    "\n",
    "Regarding the Info Dictionary, this one provides more contextual details:\n",
    "\n",
    "- **Speed:** The speed of the ego vehicle at the terminal state was approximately 23.76 units (the units for speed are not specified but could be km/h or m/s depending on the environment's configuration).\n",
    "- **Crashed:** The episode ended in a crash, indicating that the ego vehicle crashed into another vehicle or an obstacle (represented by True).\n",
    "- **Action:** The last action taken by the agent was 3, suggesting that the agent was attempting to move to the right.\n",
    "- **Rewards:**  The \"collision_reward\" of 1.0 indicates that the agent received a penalty for crashing. The \"right_lane_reward\" of 0.67 suggests that the agent was almost in the rightmost lane at the time of the crash. The \"high_speed_reward\" of 0.38 shows that the agent was partially maintaining a high speed when it crashed. The \"on_road_reward\" of 1.0 indicates that the agent was on the road until the crash."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting 1 - Reward Function 1, PPO, Hyperparameter Set 1**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part III.1: Define the Reward Function**\n",
    "\n",
    "Checking and adapting the current environment's reward function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Inspect the Current Reward Function:**\n",
    "\n",
    "Holistic view of the current environment configuration settings and reward elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'type': 'DiscreteMetaAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 40,\n",
      " 'ego_spacing': 2,\n",
      " 'high_speed_reward': 0.4,\n",
      " 'initial_lane_id': None,\n",
      " 'lane_change_reward': 0,\n",
      " 'lanes_count': 4,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'observation_shape': (128, 64),\n",
      "                 'scaling': 1.75,\n",
      "                 'stack_size': 4,\n",
      "                 'type': 'GrayscaleObservation',\n",
      "                 'weights': [0.2989, 0.587, 0.114]},\n",
      " 'offroad_terminal': False,\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 2,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'reward_speed_range': [20, 30],\n",
      " 'right_lane_reward': 0.1,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15,\n",
      " 'vehicles_count': 50,\n",
      " 'vehicles_density': 1}\n"
     ]
    }
   ],
   "source": [
    "# Checking the current environment configuration.\n",
    "pprint.pprint(env.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:** The rewards structure was built to achieve two major goals:\n",
    "\n",
    "- **Foster Safe Driving:** By penalizing collisions and rewading good driving behaviors;\n",
    "- **Foster Efficient Driving:** By rewarding high speed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure is composed by **6 different reward elements**:\n",
    "\n",
    "1. **Collision Reward:** The agent receives a reward of -1 for colliding with another vehicle. This large negative reward encourages the agent to learn to avoid collisions. \n",
    "\n",
    "2. **Right Lane Reward:** The agent gets a reward of 0.1 for driving in the right-most lanes and this reward is linearly mapped to zero for other lanes. This simulates the common traffic rule in many countries where slower traffic should keep right, and the fastest lane (for overtaking) is the left-most one. The agent is thus encouraged to maintain right unless it needs to overtake.\n",
    "\n",
    "3. **High Speed Reward:** The agent receives a reward of 0.4 for driving at the highest speed (full speed), which is linearly mapped to zero for lower speeds according to the range specified in the \"reward_speed_range\" parameter. This encourages the agent to reach its destination as quickly as possible, which is often a desired behavior in many driving scenarios. However, speed must be balanced with safety; hence the reward for avoiding collisions is higher (in absolute terms) than the reward for high speed.\n",
    "\n",
    "4. **Lane Change Reward:** The agent gets a reward of 0 for each lane change action. This could be interpreted in different ways. One possible interpretation is that lane changes, in and of themselves, are neither good nor bad, but are merely tools that can be used to achieve other objectives (like avoiding collisions or driving fast).\n",
    "\n",
    "5. **Normalization:**  The rewards are normalized, likely to keep the total reward within a certain range. and helping the stability of the learning algorithm.\n",
    "\n",
    "6. **Offroad Terminal:** In this configuration, the offroad_terminal flag is set to False, meaning the agent isn't penalized for going off-road."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Adapt Reward Function:**\n",
    "\n",
    "Adapting the current reward function to optimize and better represent a real-world scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the context of this project, which is modeling an autonomous driving ambulance that makes driving decisions based on Reinforcement Learning, some of the current reward criteria could be better adapted to resemble a real-world situation of attempting to get someone to the hospital as quickly as possible while maintaining the safety of the drivers and patients.\n",
    "\n",
    "As so, the following changes will be performed: \n",
    "\n",
    "- **Slightly Penalize Lane Changes:** Deducting **-0.05** for each lane change in order to avoid frequent unecessary lane changes that might be dangerous and disruptive to other drivers.\n",
    "\n",
    "- **Slightly Increase the High Speed Reward:** Boosting the high speed reward to **0.6**, to stimulate the urgency that Emergency Response teams should have to minimize the time of taking a patient to the hospital in a between life and death in an emergency context.\n",
    "\n",
    "- **Set the Off Road Terminal Flag to True:** Forbiding the ambulance to go off road, as this scenario is considered to be a serious dangerous driving crime in a real highway driving scenario.\n",
    "\n",
    "- **Reward Safe Distance from Other Cars:** Rewarding increases of the minimum distance to other cars, in order to make the ride safer (and to reduce the risk of collisions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a new environment class that inherits from the original HighwayEnv.\n",
    "class MyHighwayEnv(HighwayEnv):\n",
    "    # Defining the class method to provide the configurations of the environment.\n",
    "    @classmethod\n",
    "    def default_config(cls) -> dict:\n",
    "        # Getting the default configurations from the parent class.\n",
    "        config = super().default_config()\n",
    "        # Updating the default configuration with the new parameters.\n",
    "        config.update({\n",
    "            \n",
    "            # Defining the configuration for the observation space - image-based.\n",
    "            \"observation\": {\n",
    "                \"type\": \"GrayscaleObservation\",\n",
    "                \"observation_shape\": (128, 64),\n",
    "                \"stack_size\": 4,\n",
    "                \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "                \"scaling\": 1.75,\n",
    "            },\n",
    "\n",
    "            # Defining the configuration for the action space - discrete.\n",
    "            \"action\": {\n",
    "                \"type\": \"DiscreteMetaAction\",\n",
    "            },\n",
    "\n",
    "            # Defining the configuration for the reward and policy functions.\n",
    "            \"lanes_count\": 4,\n",
    "            \"vehicles_count\": 50,\n",
    "            \"controlled_vehicles\": 1,\n",
    "            \"initial_lane_id\": None,\n",
    "            \"duration\": 40,  # [s]\n",
    "            \"ego_spacing\": 2,\n",
    "            \"vehicles_density\": 1,\n",
    "\n",
    "            # Penalizing for collisions.\n",
    "            \"collision_reward\": -1,\n",
    "\n",
    "            # Rewarding good driving behavior of being on the right lane.\n",
    "            \"right_lane_reward\": 0.1,\n",
    "\n",
    "            # Increasing the high speed reward.\n",
    "            \"high_speed_reward\": 0.6,\n",
    "            \n",
    "            # Penalizing the lane changes.\n",
    "            \"lane_change_reward\": -0.05,\n",
    "\n",
    "            # Rewarding a safe distance from other cars.\n",
    "            \"safe_distance_reward\": 0.1,  \n",
    "\n",
    "            \"reward_speed_range\": [20, 30],\n",
    "\n",
    "            # Normalizing the rewards.\n",
    "            \"normalize_reward\": True,\n",
    "\n",
    "            # Prohibiting going off road.\n",
    "            \"offroad_terminal\": True,\n",
    "            \"policy_frequency\": 2\n",
    "        })\n",
    "\n",
    "        return config\n",
    "\n",
    "    # Adapting the scalar method to compute the total reward.\n",
    "    def _reward(self, action: Action) -> float:\n",
    "        \"\"\"\n",
    "        The reward is defined to foster driving at high speed, on the rightmost lanes, and to avoid collisions.\n",
    "        :param action: the last action performed\n",
    "        :return: the corresponding reward\n",
    "        \"\"\"\n",
    "        # Calling the _rewards function to get a dictionary of all individual reward components.\n",
    "        rewards = self._rewards(action)\n",
    "\n",
    "        # Fetching and summing up all the individual rewards to get the total reward.\n",
    "        # If the reward is not defined, a default value of 0 is used.\n",
    "        reward = sum(self.config.get(name, 0) * reward for name, reward in rewards.items())\n",
    "        \n",
    "        \n",
    "        # Normalizing the reward.\n",
    "        # The reward is mapped to this range based on its value relative to the sum of \"collision_reward\", \"high_speed_reward\", \"right_lane_reward\", and \"safe_distance_reward\" (if it exists).\n",
    "        if self.config[\"normalize_reward\"]:\n",
    "            reward = utils.lmap(reward,\n",
    "                                [self.config[\"collision_reward\"],\n",
    "                                self.config[\"high_speed_reward\"] + self.config[\"right_lane_reward\"] + self.config.get(\"safe_distance_reward\", 0)],\n",
    "                                [0, 1])\n",
    "        \n",
    "        # Scaling the reward by the 'on_road_reward', which is 1 if the vehicle is on the road and 0 otherwise.\n",
    "        reward *= rewards['on_road_reward']\n",
    "        return reward\n",
    "\n",
    "    # Adapting the method to compute each reward element.\n",
    "    def _rewards(self, action: Action) -> Dict[Text, float]:\n",
    "\n",
    "        # Getting the indices of all lanes that are parallel and have the same direction as the current lane.\n",
    "        neighbours = self.road.network.all_side_lanes(self.vehicle.lane_index)\n",
    "\n",
    "        # Getting the index of the target lane if the vehicle is controlled, otherwise getting the index of the current lane.\n",
    "        lane = self.vehicle.target_lane_index[2] if isinstance(self.vehicle, ControlledVehicle) \\\n",
    "            else self.vehicle.lane_index[2]\n",
    "\n",
    "\n",
    "        # Calculating the forward speed of the vehicle, which is the component of the vehicle's speed in the direction of the road.\n",
    "        forward_speed = self.vehicle.speed * np.cos(self.vehicle.heading)\n",
    "\n",
    "        # Mapping the forward speed to a range of [0, 1] based on the \"reward_speed_range\" defined in the config.\n",
    "        scaled_speed = utils.lmap(forward_speed, self.config[\"reward_speed_range\"], [0, 1])\n",
    "        \n",
    "        # Calculating the minimum distance to other vehicles.\n",
    "        min_distance = float(\"inf\")\n",
    "        for v in self.road.vehicles:\n",
    "            if v is not self.vehicle:\n",
    "                distance = np.linalg.norm(self.vehicle.position - v.position)\n",
    "                min_distance = min(min_distance, distance)\n",
    "        \n",
    "        # Returning a dictionary with the individual rewards for each component.\n",
    "        return {\n",
    "            \"collision_reward\": float(self.vehicle.crashed), # Will be 1 if the vehicle has crashed, 0 otherwise.\n",
    "            \"right_lane_reward\": lane / max(len(neighbours) - 1, 1), # The fraction of the number of lanes to the right of the current lane.\n",
    "            \"high_speed_reward\": np.clip(scaled_speed, 0, 1), # The forward speed of the vehicle scaled to the range [0, 1]\n",
    "            \"on_road_reward\": float(self.vehicle.on_road), # Will be 1 if the vehicle is on the road (self.vehicle.on_road is True) and 0 otherwise.\n",
    "            \"safe_distance_reward\": utils.lmap(min_distance, [0, 100], [0, 1])  # Scale the minimum distance to other vehicles (min_distance) to the range [0, 1] over a range of 0 to 100 meters. \n",
    "        }\n",
    "    \n",
    "    # Configuring the render mode.\n",
    "    def configure(self, render_mode=\"human\"):\n",
    "        self.config[\"render_mode\"] = render_mode\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Test the Reward Function with a Random Policy:**\n",
    "\n",
    "Letting the agent take random actions in the environment with the new reward function, in order to give a baseline of what to expect from an untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the environment with the reward function changes.\n",
    "env = MyHighwayEnv(render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Episode:1 Score:20.03176854357967'\n",
      "'Episode:2 Score:4.396648019345016'\n",
      "'Episode:3 Score:63.96622908778545'\n",
      "'Episode:4 Score:29.829702179351496'\n",
      "'Episode:5 Score:7.060643814336156'\n",
      "'Episode:6 Score:8.889712304068297'\n",
      "'Episode:7 Score:20.77650157176086'\n",
      "'Episode:8 Score:6.121075766847082'\n",
      "'Episode:9 Score:33.280546051376184'\n",
      "'Episode:10 Score:3.081906896990797'\n",
      "Average Reward: 19.743473423544096\n"
     ]
    }
   ],
   "source": [
    "# Defining the number of episodes - number of times the game will restart itself when:\n",
    "#   A. The car crashes.\n",
    "#   B. The car goes outside of the track.\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "# Initializing an empty list to all generated data.\n",
    "data = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "\n",
    "    # Initializing an empty list for the current episode data.\n",
    "    episode_data = []\n",
    "    \n",
    "    # Reseting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "    \n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0 \n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "        \n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Storing the [observation, action, reward, next observation, final state] data.\n",
    "        data.append([state, action, reward, n_state, done])\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Setting the current state to the new state.\n",
    "        state = n_state\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    pprint.pprint(f'Episode:{episode} Score:{score}')\n",
    "    \n",
    "# Closing the environment.\n",
    "env.close()\n",
    "\n",
    "# Calculating the average reward over the 10 episodes.\n",
    "total_reward = sum(data_point[2] for data_point in data)\n",
    "average_reward = total_reward / episodes\n",
    "\n",
    "# Printing the average reward.\n",
    "print(\"Average Reward:\", average_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As seen, without properly training the agent to appropriatly define the right actions to take, the accumulated final scores will be very poor (average = 19.74). \n",
    "\n",
    "Despite the possible improvements that were made to the reward function (given the explored problem setting), in order to improve these scores the agent must be trained to:\n",
    "\n",
    "1. **Gather samples from the gameplay (i.e., running the policy);**\n",
    "2. **Fit a model that estimates the total final return;**\n",
    "3. **Improving the policy, in order to generate at each step an action with the end goal of maximizing the final acuumulated reward.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part IV.1: Prepare the Agent**\n",
    "\n",
    "Creating a wrapper and an agent to test a random policy for the discrete environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Set up TensorBoard Log Directory:**\n",
    "\n",
    "Setting up TensorBoard in order to later collect important model statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f'Training/Saved Models/PPO/Highway-{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "log_dir = f'Training/logs/PPO/Highway-{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    \n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the environment with the reward function changes.\n",
    "env = MyHighwayEnv(render_mode = \"human\")\n",
    "\n",
    "# Creating a Monitor wrapper to log the training data.\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Wraping the environment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** In this case, since the current action space was already discrete (5 actions) and the observation configuration was already a set of 4 sequential grayscale images , the only needed wrapper will be the one to make it compatible with the Stable Baselines3 library for importing the agent. Wrapping it again and running the same tests would be redundant since this was already done before and nothing else (like the agent or policy) changed since then.\n",
    "\n",
    "In this way, the agent will be later configured once the Policy is defined."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part V.1: Prepare the Policy Optizimation Method - Proximal Policy Optimization (PPO)**\n",
    "\n",
    "Preparing the PPO algorithm and the CNN for the training stage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Setup the Agent and Policy**\n",
    "\n",
    "Creating an agent using the **Proximal Policy Optimization** algorithm.\n",
    "\n",
    "This algorithm was built in order to optimize the decisions made by an agent in a given environment in order to maximize the notion of cumulative reward. The idea behind consists of taking steps in the direction that improves the policy, but not too large ones that might harm the performance. In other words, it seeks an optimal policy that is close to the current policy. \n",
    "\n",
    "Mathematically, PPO aims to find the optimal policy π(θ) that maximizes the objective:\n",
    "<center><img src=\"images/ppo_formula.png\"/></center>\n",
    "\n",
    "The main innovation of the this algorithm is the introduction of a \"Clipping\" mechanism that penalizes changes in the policy that deviate too much from the curren policy.\n",
    "\n",
    "In terms of the Policy, \"CNNPolicy\" - a policy architecture option that is especially good at processing grid-like data (i.e., images) was chosen. This CNN, particularly good for game environments, will take in the image-based observation from the environment, process it through its layers to detect important features, and then output an action (or a distribution over actions) that the agent should take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = PPO(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1,\n",
    "            # Defining the discount factor for future rewards (how much the agent cares about rewards in the distance future relative to immediate ones).\n",
    "            gamma=0.99,  \n",
    "            # Defining the trade-off between exploration (trying out new actions) and exploitation (sticking to what's known to work)\n",
    "            ent_coef=0.01, \n",
    "            # Defining the learning rate for the optimizer.\n",
    "            learning_rate=0.00025, \n",
    "            # Defining the value function coefficient - scaling factor to change the contribution of the value function loss to the total loss function.\n",
    "            vf_coef=0.5, \n",
    "            # Defining the maximum value for the gradient clipping.\n",
    "            max_grad_norm=0.5,\n",
    "            # Defining the trade-off between variance and bias - technique to reduce variance.\n",
    "            gae_lambda=0.95,\n",
    "            # Defining the clipping parameter for the policy update - how much the new policy can deviate from the old policy during each update.\n",
    "            clip_range=0.2,\n",
    "            # Updating the tensorboard log directory. \n",
    "            tensorboard_log=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = PPO(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Model policy CNN Architecture.\n",
    "model.policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Test the New Policy and Agent:**\n",
    "\n",
    "Testing the New Policy with the previously configured agent, without any training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[2.9394882]\n",
      "Episode:2 Score:[49.511543]\n",
      "Episode:3 Score:[4.51984]\n",
      "Episode:4 Score:[8.436631]\n",
      "Episode:5 Score:[10.931718]\n",
      "Episode:6 Score:[18.60847]\n",
      "Episode:7 Score:[4.2792044]\n",
      "Episode:8 Score:[26.44286]\n",
      "Episode:9 Score:[15.164019]\n",
      "Episode:10 Score:[46.941734]\n",
      "Average Reward: [18.777552]\n"
     ]
    }
   ],
   "source": [
    "# Running multiple episodes and record the scores and time steps.\n",
    "episodes = 10\n",
    "scores_array = []\n",
    "timestep_arr = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    # Resetting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0\n",
    "\n",
    "    # Initializing the timestep as 0.\n",
    "    timestep = 0\n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "\n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Incrementing the timesteps.\n",
    "        timestep += 1\n",
    "        \n",
    "        # Render the environment.\n",
    "        env.render()\n",
    "    \n",
    "    # Saving both the scores and timesteps.\n",
    "    scores_array.append(score)\n",
    "    timestep_arr.append(timestep)\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()\n",
    "\n",
    "# Calculating the average reward over the 10 episodes.\n",
    "total_reward = sum(scores_array)\n",
    "average_reward = total_reward / episodes\n",
    "\n",
    "# Printing the average reward.\n",
    "print(\"Average Reward:\", average_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** Although the improvements on the actions that the car takes are noticeable, it is clear that the agent is definetly not trained as given for a specific environment setting the car actions aren't yet perfect. Given such poor generated scores, the hypothesis that can be raised is that with a larger amount of training steps, the agent will be able to learn a better policy in order to take actions that will maximize the accumulated score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VI.1: Training Stage - Proximal Policy Optimization (PPO)**\n",
    "\n",
    "Now that the environment, agent and policy are properly configured and after confirming the poor results from not training the agent, the PPO/CNN Policy model combination will be trained for 2 longer periods of time to see if this will improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a callback to write summaries in tensorboard during training.\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, models_dir: str, verbose=1):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.models_dir = models_dir\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean reward of the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                self.logger.record('rollout/mean_reward', float(mean_reward))\n",
    "                \n",
    "                # Save model\n",
    "                save_path = os.path.join(self.models_dir, f'model_step_{self.num_timesteps}')\n",
    "                self.model.save(save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Creating the TensorBoardCallback instance.\n",
    "callback = TensorboardCallback(check_freq=5000, log_dir=log_dir, models_dir=models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the short model timesteps - 20K.\n",
    "#model.learn(total_timesteps=20000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model.\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VII.1: Evaluation and Testing Stages**\n",
    "\n",
    "Evaluating the performance of the testing performed with the long timestepped model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the environment with the reward function changes.\n",
    "env = MyHighwayEnv(render_mode = \"human\")\n",
    "\n",
    "# Wraping the nevironment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Load the Model:**\n",
    "\n",
    "Loading the longer trained model from its orginal path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model, based on the data gathered during the training stage, will be loaded around step 20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO', 'Highway-2023-07-02_17-58-25', 'model_step_20000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading the model to see if it was properly saved.\n",
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Evaluation:**\n",
    "\n",
    "This step is done to measure the performance of the trained agent. Evaluation gives us an estimate of how well the agent will perform and can be used to compare different models or configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48.15267017483711, 1.393639236688614)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the performance of the best trained model.\n",
    "evaluate = evaluate_policy(model, env, n_eval_episodes=2, render=True)\n",
    "print(evaluate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Testing:**\n",
    "\n",
    "This step is done as a final assessment of the agent, in order to check how it would operate in the real world given the model that it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[46.979393]\n",
      "Episode:2 Score:[45.627705]\n",
      "Episode:3 Score:[45.950012]\n",
      "Episode:4 Score:[48.068104]\n",
      "Episode:5 Score:[47.29076]\n",
      "Episode:6 Score:[47.010593]\n",
      "Episode:7 Score:[47.80622]\n",
      "Episode:8 Score:[47.070576]\n",
      "Episode:9 Score:[46.227947]\n",
      "Episode:10 Score:[46.999123]\n",
      "Average Reward: [46.90304]\n"
     ]
    }
   ],
   "source": [
    "# Running multiple episodes and record the scores and time steps.\n",
    "episodes = 10\n",
    "scores_array = []\n",
    "timestep_arr = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    # Resetting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0\n",
    "\n",
    "    # Initializing the timestep as 0.\n",
    "    timestep = 0\n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "\n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Incrementing the timesteps.\n",
    "        timestep += 1\n",
    "        \n",
    "        # Render the environment.\n",
    "        env.render()\n",
    "    \n",
    "    # Saving both the scores and timesteps.\n",
    "    scores_array.append(score)\n",
    "    timestep_arr.append(timestep)\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()\n",
    "\n",
    "# Calculating the average reward over the 10 episodes.\n",
    "total_reward = sum(scores_array)\n",
    "average_reward = total_reward / episodes\n",
    "\n",
    "# Printing the average reward.\n",
    "print(\"Average Reward:\", average_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VIII.1: Score Analysis**\n",
    "\n",
    "Plotting the scores and time steps for each episode to analyze the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the scores and timestep per episode.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_array)+1), scores_array, label=\"Score\", color='blue')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,)\n",
    "plt.xticks(np.arange(1, len(scores_array)+1))\n",
    "plt.xlabel('Episodes #')\n",
    "plt.title(\"Scores over Episodes\", fontweight='bold' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As observable, with the longest trained and best performing model the results far exceeded the ones from the first testing stages. Despite not breaking the 50 points goal that was set in the beginning of the experience, the scores have shown to be very regular which showcases the model robustness to different environments and the opportunity to improve it after some proper hyperparameter tuning or a different optimizer choice. Furthermore, the fact that the car rarely attempted to overcome someone may indicate the inneficiency of the safe distance and lane change rewards. As such, these ones will be adapted in the next setting test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting 2 - Reward Function 2, PPO, Hyperparameter Set 2**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part III.2: Define the Reward Function**\n",
    "\n",
    "Checking and adapting the current environment's reward function after the observations on Setting 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Adapt Reward Function:**\n",
    "\n",
    "Adapting the current reward function given the behaviors observed in setting 1 to optimize and better represent a real-world scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the context of this project and the behaviors observed in Setting 1, some of the previous reward criteria could be better adapted to resemble a real-world situation of attempting to get someone to the hospital as quickly as possible while maintaining the safety of the drivers and patients.\n",
    "\n",
    "As so, the following changes will be performed:\n",
    "\n",
    "- **Reducing the Penalty for Lane Changes:** Reducing the previous configured penalty to **-0.04**, in order to incentivize the ambulance to overcome other cars instead of remaining static in its own lane.\n",
    "\n",
    "- **Slightly Increase the High Speed Reward:** Boosting the high speed reward to **0.7**, to stimulate even more the urgency that Emergency Response teams should have to minimize the time of taking a patient to the hospital in a between life and death in an emergency context.\n",
    "\n",
    "- **Remove the Safe Distance Reward:** Removing the safe distance reward in order to stimulate the ambulance to drive faster and overcome some of the other cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a new environment class that inherits from the original HighwayEnv.\n",
    "class MyHighwayEnv(HighwayEnv):\n",
    "    # Defining the class method to provide the configurations of the environment.\n",
    "    @classmethod\n",
    "    def default_config(cls) -> dict:\n",
    "        # Getting the default configurations from the parent class.\n",
    "        config = super().default_config()\n",
    "        # Updating the default configuration with the new parameters.\n",
    "        config.update({\n",
    "            \n",
    "            # Defining the configuration for the observation space - image-based.\n",
    "            \"observation\": {\n",
    "                \"type\": \"GrayscaleObservation\",\n",
    "                \"observation_shape\": (128, 64),\n",
    "                \"stack_size\": 4,\n",
    "                \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "                \"scaling\": 1.75,\n",
    "            },\n",
    "\n",
    "            # Defining the configuration for the action space - discrete.\n",
    "            \"action\": {\n",
    "                \"type\": \"DiscreteMetaAction\",\n",
    "            },\n",
    "\n",
    "            # Defining the configuration for the reward and policy functions.\n",
    "            \"lanes_count\": 4,\n",
    "            \"vehicles_count\": 50,\n",
    "            \"controlled_vehicles\": 1,\n",
    "            \"initial_lane_id\": None,\n",
    "            \"duration\": 40,  # [s]\n",
    "            \"ego_spacing\": 2,\n",
    "            \"vehicles_density\": 1,\n",
    "\n",
    "            # Penalizing for collisions.\n",
    "            \"collision_reward\": -1,\n",
    "\n",
    "            # Rewarding good driving behavior of being on the right lane.\n",
    "            \"right_lane_reward\": 0.1,\n",
    "\n",
    "            # Increasing the high speed reward comparing to setting 1.\n",
    "            \"high_speed_reward\": 0.7,\n",
    "            \n",
    "            # Reducing the penalty for lane changes.\n",
    "            \"lane_change_reward\": -0.04,\n",
    "\n",
    "            # Removing the safe distance reward.\n",
    "            #\"safe_distance_reward\": 0.1,  \n",
    "\n",
    "            \"reward_speed_range\": [20, 30],\n",
    "\n",
    "            # Normalizing the rewards.\n",
    "            \"normalize_reward\": True,\n",
    "\n",
    "            # Prohibiting going off road.\n",
    "            \"offroad_terminal\": True,\n",
    "            \"policy_frequency\": 2\n",
    "        })\n",
    "\n",
    "        return config\n",
    "\n",
    "    # Adapting the scalar method to compute the total reward.\n",
    "    def _reward(self, action: Action) -> float:\n",
    "        \"\"\"\n",
    "        The reward is defined to foster driving at high speed, on the rightmost lanes, and to avoid collisions.\n",
    "        :param action: the last action performed\n",
    "        :return: the corresponding reward\n",
    "        \"\"\"\n",
    "        # Calling the _rewards function to get a dictionary of all individual reward components.\n",
    "        rewards = self._rewards(action)\n",
    "\n",
    "        # Fetching and summing up all the individual rewards to get the total reward.\n",
    "        # If the reward is not defined, a default value of 0 is used.\n",
    "        reward = sum(self.config.get(name, 0) * reward for name, reward in rewards.items())\n",
    "        \n",
    "        # Normalizing the reward.\n",
    "        # The reward is mapped to this range based on its value relative to the sum of \"collision_reward\", \"high_speed_reward\", \"right_lane_reward\".\n",
    "        if self.config[\"normalize_reward\"]:\n",
    "            reward = utils.lmap(reward,\n",
    "                                [self.config[\"collision_reward\"],\n",
    "                                self.config[\"high_speed_reward\"] + self.config[\"right_lane_reward\"]],\n",
    "                                [0, 1])\n",
    "        \n",
    "        # Scaling the reward by the 'on_road_reward', which is 1 if the vehicle is on the road and 0 otherwise.\n",
    "        reward *= rewards['on_road_reward']\n",
    "        return reward\n",
    "\n",
    "    # Adapting the method to compute each reward element.\n",
    "    def _rewards(self, action: Action) -> Dict[Text, float]:\n",
    "\n",
    "        # Getting the indices of all lanes that are parallel and have the same direction as the current lane.\n",
    "        neighbours = self.road.network.all_side_lanes(self.vehicle.lane_index)\n",
    "\n",
    "        # Getting the index of the target lane if the vehicle is controlled, otherwise getting the index of the current lane.\n",
    "        lane = self.vehicle.target_lane_index[2] if isinstance(self.vehicle, ControlledVehicle) \\\n",
    "            else self.vehicle.lane_index[2]\n",
    "\n",
    "\n",
    "        # Calculating the forward speed of the vehicle, which is the component of the vehicle's speed in the direction of the road.\n",
    "        forward_speed = self.vehicle.speed * np.cos(self.vehicle.heading)\n",
    "\n",
    "        # Mapping the forward speed to a range of [0, 1] based on the \"reward_speed_range\" defined in the config.\n",
    "        scaled_speed = utils.lmap(forward_speed, self.config[\"reward_speed_range\"], [0, 1])\n",
    "        \n",
    "        # Returning a dictionary with the individual rewards for each component.\n",
    "        return {\n",
    "            \"collision_reward\": float(self.vehicle.crashed), # Will be 1 if the vehicle has crashed, 0 otherwise.\n",
    "            \"right_lane_reward\": lane / max(len(neighbours) - 1, 1), # The fraction of the number of lanes to the right of the current lane.\n",
    "            \"high_speed_reward\": np.clip(scaled_speed, 0, 1), # The forward speed of the vehicle scaled to the range [0, 1]\n",
    "            \"on_road_reward\": float(self.vehicle.on_road), # Will be 1 if the vehicle is on the road (self.vehicle.on_road is True) and 0 otherwise. \n",
    "        }\n",
    "    \n",
    "    # Configuring the render mode.\n",
    "    def configure(self, render_mode=\"human\"):\n",
    "        self.config[\"render_mode\"] = render_mode\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Test the Reward Function with a Random Policy:**\n",
    "\n",
    "Letting the agent take random actions in the environment with the new reward function, in order to give a baseline of what to expect from an untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the environment with the reward function changes.\n",
    "env = MyHighwayEnv(render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Episode:1 Score:22.39766686845734'\n",
      "'Episode:2 Score:23.786542788524276'\n",
      "'Episode:3 Score:22.060077537378564'\n",
      "'Episode:4 Score:16.083087966932162'\n",
      "'Episode:5 Score:15.91375381546268'\n",
      "'Episode:6 Score:18.716107851581015'\n",
      "'Episode:7 Score:20.55436889739848'\n",
      "'Episode:8 Score:9.784913693411342'\n",
      "'Episode:9 Score:18.449149471357035'\n",
      "'Episode:10 Score:16.861686083205587'\n",
      "Average Reward: 18.46073549737087\n"
     ]
    }
   ],
   "source": [
    "# Defining the number of episodes - number of times the game will restart itself when:\n",
    "#   A. The car crashes.\n",
    "#   B. The car goes outside of the track.\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "# Initializing an empty list to all generated data.\n",
    "data = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "\n",
    "    # Initializing an empty list for the current episode data.\n",
    "    episode_data = []\n",
    "    \n",
    "    # Reseting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "    \n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0 \n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "        \n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Storing the [observation, action, reward, next observation, final state] data.\n",
    "        data.append([state, action, reward, n_state, done])\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Setting the current state to the new state.\n",
    "        state = n_state\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    pprint.pprint(f'Episode:{episode} Score:{score}')\n",
    "    \n",
    "# Closing the environment.\n",
    "env.close()\n",
    "\n",
    "# Calculating the average reward over the 10 episodes.\n",
    "total_reward = sum(data_point[2] for data_point in data)\n",
    "average_reward = total_reward / episodes\n",
    "\n",
    "# Printing the average reward.\n",
    "print(\"Average Reward:\", average_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As seen, still without properly training the agent to appropriatly define the right actions to take, the accumulated final scores will be very poor (average = 20.48). \n",
    "\n",
    "Despite the possible improvements that were made to the reward function (given the explored problem setting and some of the corrections from setting 1), in order to improve these scores the agent must be trained to:\n",
    "\n",
    "1. **Gather samples from the gameplay (i.e., running the policy);**\n",
    "2. **Fit a model that estimates the total final return;**\n",
    "3. **Improving the policy, in order to generate at each step an action with the end goal of maximizing the final acuumulated reward.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part IV.2: Prepare the Agent**\n",
    "\n",
    "Creating a wrapper and an agent to test a random policy for the discrete environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Set up TensorBoard Log Directory:**\n",
    "\n",
    "Setting up TensorBoard in order to later collect important model statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir2 = f'Training/Saved Models/DQN/Highway2-{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "log_dir2 = f'Training/logs/DQN/Highway2-{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "if not os.path.exists(models_dir2):\n",
    "    os.makedirs(models_dir2)\n",
    "    \n",
    "if not os.path.exists(log_dir2):\n",
    "    os.makedirs(log_dir2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the environment with the reward function changes.\n",
    "env = MyHighwayEnv(render_mode = \"human\")\n",
    "\n",
    "# Creating a Monitor wrapper to log the training data.\n",
    "env = Monitor(env, log_dir2)\n",
    "\n",
    "# Wraping the environment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** In this case, since the current action space was already discrete (5 actions) and the observation configuration was already a set of 4 sequential grayscale images , the only needed wrapper will be the one to make it compatible with the Stable Baselines3 library for importing the agent. Wrapping it again and running the same tests would be redundant since this was already done before and nothing else (like the agent or policy) changed since then.\n",
    "\n",
    "In this way, the agent will be later configured once the Policy is defined."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part V.1: Prepare the Policy Optimization Method - DQN**\n",
    "\n",
    "Preparing the dqn algorithm and the CNN for the training stage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Setup the Agent and Policy**\n",
    "\n",
    "Creating an agent using the **Deep Q-Network** algorithm.\n",
    "\n",
    "This algorithm takes the classic Q-learning technique and combines with deep neural networks. DQN aims to find the best action to take in a given state in order to maximize the expected cumulative reward. It achieves this by learning an optimal policy π(θ) that yields the highest Q-value, representing the best expected future reward for a given action in a state.\n",
    "\n",
    "<center><img src=\"images/q_dqn_formula.png\"/></center>\n",
    "\n",
    "The true innovation of DQN is the combination of Q-learning with a flexible function approximator through deep learning. This powerful combination allows DQN to handle high-dimensional observation spaces that are typically not feasible with classic Q-learning.\n",
    "\n",
    "In terms of the Policy, the same \"CNNPolicy\" - a policy architecture option that is especially good at processing grid-like data (i.e., images) was chosen. This CNN, particularly good for game environments, will take in the image-based observation from the environment, process it through its layers to detect important features, and then output an action (or a distribution over actions) that the agent should take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the previous model from memory.\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = DQN(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1,\n",
    "            # Defining the discount factor for future rewards (how much the agent cares about rewards in the distance future relative to immediate ones).\n",
    "            gamma=0.70,\n",
    "            # Lowering the buffer_size.\n",
    "            buffer_size=10000,\n",
    "            # Defining the learning rate for the optimizer.\n",
    "            learning_rate=0.001,\n",
    "            # Defining the initial exploration rate.\n",
    "            exploration_initial_eps = 1.0,\n",
    "            # Updating the tensorboard log directory. \n",
    "            tensorboard_log=log_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CnnPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): NatureCNN(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): NatureCNN(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Model policy CNN Architecture.\n",
    "model.policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Test the New Policy and Agent:**\n",
    "\n",
    "Testing the New Policy with the previously configured agent, without any training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[59.96798]\n",
      "Episode:2 Score:[31.480654]\n",
      "Episode:3 Score:[15.729358]\n",
      "Episode:4 Score:[59.98243]\n",
      "Episode:5 Score:[31.62963]\n",
      "Episode:6 Score:[37.53582]\n",
      "Episode:7 Score:[12.194445]\n",
      "Episode:8 Score:[37.48243]\n",
      "Episode:9 Score:[32.34761]\n",
      "Episode:10 Score:[37.59761]\n",
      "Average Reward: [35.594795]\n"
     ]
    }
   ],
   "source": [
    "# Running multiple episodes and record the scores and time steps.\n",
    "episodes = 10\n",
    "scores_array = []\n",
    "timestep_arr = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    # Resetting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0\n",
    "\n",
    "    # Initializing the timestep as 0.\n",
    "    timestep = 0\n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "\n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Incrementing the timesteps.\n",
    "        timestep += 1\n",
    "        \n",
    "        # Render the environment.\n",
    "        env.render()\n",
    "    \n",
    "    # Saving both the scores and timesteps.\n",
    "    scores_array.append(score)\n",
    "    timestep_arr.append(timestep)\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()\n",
    "\n",
    "# Calculating the average reward over the 10 episodes.\n",
    "total_reward = sum(scores_array)\n",
    "average_reward = total_reward / episodes\n",
    "\n",
    "# Printing the average reward.\n",
    "print(\"Average Reward:\", average_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** Initially, the effort of the car to colect the points for staying in the same lane is very noticeable although this action makes it crash very soon in almost every single environment. Given such poor generated scores, the hypothesis that can be raised is once again that with a larger amount of training steps, the agent will be able to learn a better policy in order to take actions that will maximize the accumulated score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VI.2: Training Stage - DQN**\n",
    "\n",
    "Now that the environment, agent and policy are properly configured and after confirming the poor results from not training the agent, the DQN/CNN Policy model combination will be trained for 10 longer periods of time to see if this will improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a callback to write summaries in tensorboard during training.\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, models_dir: str, verbose=1):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.models_dir = models_dir\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean reward of the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                self.logger.record('rollout/mean_reward', float(mean_reward))\n",
    "                \n",
    "                # Save model\n",
    "                save_path = os.path.join(self.models_dir, f'model_step_{self.num_timesteps}')\n",
    "                self.model.save(save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Creating the TensorBoardCallback instance.\n",
    "callback = TensorboardCallback(check_freq=2000, log_dir=log_dir2, models_dir=models_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/logs/DQN/Highway2-2023-07-03_01-03-58\\DQN_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29       |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 116      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28       |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 224      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31       |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.647    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total_timesteps  | 372      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.4     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.553    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total_timesteps  | 471      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.422    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 608      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.7     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.346    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 301      |\n",
      "|    total_timesteps  | 688      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.6     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.238    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 350      |\n",
      "|    total_timesteps  | 802      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.5     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.195    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 370      |\n",
      "|    total_timesteps  | 847      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.123    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 402      |\n",
      "|    total_timesteps  | 923      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 18.8     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 455      |\n",
      "|    total_timesteps  | 1047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.7     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 508      |\n",
      "|    total_timesteps  | 1173     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.3     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 546      |\n",
      "|    total_timesteps  | 1261     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 581      |\n",
      "|    total_timesteps  | 1346     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 634      |\n",
      "|    total_timesteps  | 1470     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 659      |\n",
      "|    total_timesteps  | 1528     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 714      |\n",
      "|    total_timesteps  | 1659     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 740      |\n",
      "|    total_timesteps  | 1720     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 789      |\n",
      "|    total_timesteps  | 1837     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 817      |\n",
      "|    total_timesteps  | 1902     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 853      |\n",
      "|    total_timesteps  | 1987     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 18.2     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 882      |\n",
      "|    total_timesteps  | 2055     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 954      |\n",
      "|    total_timesteps  | 2226     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1006     |\n",
      "|    total_timesteps  | 2349     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1049     |\n",
      "|    total_timesteps  | 2449     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1068     |\n",
      "|    total_timesteps  | 2495     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1106     |\n",
      "|    total_timesteps  | 2583     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1144     |\n",
      "|    total_timesteps  | 2673     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 17.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1175     |\n",
      "|    total_timesteps  | 2746     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1213     |\n",
      "|    total_timesteps  | 2836     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1238     |\n",
      "|    total_timesteps  | 2896     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1266     |\n",
      "|    total_timesteps  | 2961     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 16.1     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1282     |\n",
      "|    total_timesteps  | 3000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1311     |\n",
      "|    total_timesteps  | 3068     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1358     |\n",
      "|    total_timesteps  | 3180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1388     |\n",
      "|    total_timesteps  | 3253     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1412     |\n",
      "|    total_timesteps  | 3308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1466     |\n",
      "|    total_timesteps  | 3436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1519     |\n",
      "|    total_timesteps  | 3563     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1550     |\n",
      "|    total_timesteps  | 3636     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1561     |\n",
      "|    total_timesteps  | 3661     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1612     |\n",
      "|    total_timesteps  | 3783     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1656     |\n",
      "|    total_timesteps  | 3887     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1700     |\n",
      "|    total_timesteps  | 3991     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 15.5     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1752     |\n",
      "|    total_timesteps  | 4116     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1769     |\n",
      "|    total_timesteps  | 4156     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1794     |\n",
      "|    total_timesteps  | 4215     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1820     |\n",
      "|    total_timesteps  | 4276     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1867     |\n",
      "|    total_timesteps  | 4387     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1917     |\n",
      "|    total_timesteps  | 4504     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1963     |\n",
      "|    total_timesteps  | 4615     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2010     |\n",
      "|    total_timesteps  | 4726     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2073     |\n",
      "|    total_timesteps  | 4876     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2120     |\n",
      "|    total_timesteps  | 4988     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 16.3     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2163     |\n",
      "|    total_timesteps  | 5088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 17       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2235     |\n",
      "|    total_timesteps  | 5261     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2290     |\n",
      "|    total_timesteps  | 5390     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2325     |\n",
      "|    total_timesteps  | 5474     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2347     |\n",
      "|    total_timesteps  | 5525     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2374     |\n",
      "|    total_timesteps  | 5590     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2418     |\n",
      "|    total_timesteps  | 5693     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 18.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2464     |\n",
      "|    total_timesteps  | 5801     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2490     |\n",
      "|    total_timesteps  | 5864     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 18.2     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2554     |\n",
      "|    total_timesteps  | 6016     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2623     |\n",
      "|    total_timesteps  | 6180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2653     |\n",
      "|    total_timesteps  | 6252     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2694     |\n",
      "|    total_timesteps  | 6348     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2733     |\n",
      "|    total_timesteps  | 6441     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2785     |\n",
      "|    total_timesteps  | 6564     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2819     |\n",
      "|    total_timesteps  | 6645     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2861     |\n",
      "|    total_timesteps  | 6743     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.1     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2896     |\n",
      "|    total_timesteps  | 6826     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2927     |\n",
      "|    total_timesteps  | 6899     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2952     |\n",
      "|    total_timesteps  | 6960     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 18.7     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2987     |\n",
      "|    total_timesteps  | 7042     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3018     |\n",
      "|    total_timesteps  | 7115     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3047     |\n",
      "|    total_timesteps  | 7185     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3074     |\n",
      "|    total_timesteps  | 7247     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3109     |\n",
      "|    total_timesteps  | 7330     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 17.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3161     |\n",
      "|    total_timesteps  | 7453     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3228     |\n",
      "|    total_timesteps  | 7614     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3279     |\n",
      "|    total_timesteps  | 7734     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3313     |\n",
      "|    total_timesteps  | 7813     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3334     |\n",
      "|    total_timesteps  | 7863     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 17.7     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3393     |\n",
      "|    total_timesteps  | 8003     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3453     |\n",
      "|    total_timesteps  | 8145     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3490     |\n",
      "|    total_timesteps  | 8234     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3514     |\n",
      "|    total_timesteps  | 8290     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3537     |\n",
      "|    total_timesteps  | 8344     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3572     |\n",
      "|    total_timesteps  | 8427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 16.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3610     |\n",
      "|    total_timesteps  | 8518     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3641     |\n",
      "|    total_timesteps  | 8591     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3676     |\n",
      "|    total_timesteps  | 8674     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3729     |\n",
      "|    total_timesteps  | 8800     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3770     |\n",
      "|    total_timesteps  | 8896     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "|    mean_reward      | 16.3     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3819     |\n",
      "|    total_timesteps  | 9013     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3877     |\n",
      "|    total_timesteps  | 9151     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3892     |\n",
      "|    total_timesteps  | 9186     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3932     |\n",
      "|    total_timesteps  | 9280     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3958     |\n",
      "|    total_timesteps  | 9342     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4015     |\n",
      "|    total_timesteps  | 9477     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 17       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4037     |\n",
      "|    total_timesteps  | 9530     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4087     |\n",
      "|    total_timesteps  | 9647     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.2     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4173     |\n",
      "|    total_timesteps  | 9851     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4217     |\n",
      "|    total_timesteps  | 9957     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x174e92ba250>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the short model timesteps - 10K.\n",
    "# model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model.\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VII.2: Evaluation and Testing Stages**\n",
    "\n",
    "Evaluating the performance of the testing performed with the long timestepped model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the environment with the reward function changes.\n",
    "env = MyHighwayEnv(render_mode = \"human\")\n",
    "\n",
    "# Wraping the nevironment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Load the Model:**\n",
    "\n",
    "Loading the longer trained model from its orginal path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model, based on the data gathered during the training stage, will be loaded around step 6000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "DQN_path = os.path.join('Training', 'Saved Models', 'DQN', 'Highway2-2023-07-03_01-03-58', 'model_step_6000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to see if it was properly saved.\n",
    "model = DQN.load(DQN_path, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Evaluation:**\n",
    "\n",
    "This step is done to measure the performance of the trained agent. Evaluation gives us an estimate of how well the agent will perform and can be used to compare different models or configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.237361473962665, 2.8540124390274286)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the performance of the best trained model.\n",
    "evaluate = evaluate_policy(model, env, n_eval_episodes=2, render=True)\n",
    "print(evaluate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Testing:**\n",
    "\n",
    "This step is done as a final assessment of the agent, in order to check how it would operate in the real world given the model that it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[12.121318]\n",
      "Episode:2 Score:[61.487732]\n",
      "Episode:3 Score:[6.5500154]\n",
      "Episode:4 Score:[27.981916]\n",
      "Episode:5 Score:[17.708717]\n",
      "Episode:6 Score:[28.911121]\n",
      "Episode:7 Score:[11.458719]\n",
      "Episode:8 Score:[29.729351]\n",
      "Episode:9 Score:[3.208781]\n",
      "Episode:10 Score:[47.468105]\n",
      "Average Reward: [24.662579]\n"
     ]
    }
   ],
   "source": [
    "# Running multiple episodes and record the scores and time steps.\n",
    "episodes = 10\n",
    "scores_array = []\n",
    "timestep_arr = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    # Resetting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0\n",
    "\n",
    "    # Initializing the timestep as 0.\n",
    "    timestep = 0\n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "\n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Incrementing the timesteps.\n",
    "        timestep += 1\n",
    "        \n",
    "        # Render the environment.\n",
    "        env.render()\n",
    "    \n",
    "    # Saving both the scores and timesteps.\n",
    "    scores_array.append(score)\n",
    "    timestep_arr.append(timestep)\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()\n",
    "\n",
    "# Calculating the average reward over the 10 episodes.\n",
    "total_reward = sum(scores_array)\n",
    "average_reward = total_reward / episodes\n",
    "\n",
    "# Printing the average reward.\n",
    "print(\"Average Reward:\", average_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VIII.2: Score Analysis**\n",
    "\n",
    "Plotting the scores and time steps for each episode to analyze the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the scores and timestep per episode.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_array)+1), scores_array, label=\"Score\", color='lightblue')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,)\n",
    "plt.xticks(np.arange(1, len(scores_array)+1))\n",
    "plt.xlabel('Episodes #')\n",
    "plt.title(\"Scores over Episodes\", fontweight='bold' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:** Analyzing the scores from the DQN model reveals some interesting patterns and highlights areas for improvement. Firstly, the variation in scores across episodes indicates a degree of inconsistency in the model's performance. Despite some impressive scores, such as 61.48 in episode 2, there are instances of lower scores, for instance, 3.21 in episode 9. This discrepancy suggests that the model's current strategy of veering to the right to accrue points often leads to crashes, undermining the overall performance. The model's preference for moving right appears to be an exploitation of the current reward system, indicating a need to adjust this system to discourage risky maneuvers. It's also worth noting that the model's inability to consistently achieve high scores might be due to inefficient reward mechanisms for maintaining safe distance and appropriate lane changes. Thus, to improve the model's performance, changes in these reward structures, as well as a more balanced reward system for different driving maneuvers, could be beneficial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part IX: Model Comparison**\n",
    "\n",
    "Assessing the performance and stability of the tested models - Setting 1 and 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🥇 Average Reward:**\n",
    "\n",
    "| Setting 1 (PPO, 20000 timesteps, Reward Function 1, Hyperparameter Set 1) | Setting 2 (DQN, 6000 timesteps, Reward Function 2, Hyperparameter Set 2) |\n",
    "| --- | --- |\n",
    "| 46.90 points | 26.67 points |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📈 Standard Deviation (Measure of Reward Stability):**\n",
    "\n",
    "| Setting 1 (PPO, Reward Function 1, Hyperparameter Set 1) | Setting 2 (DQN, Reward Function 2, Hyperparameter Set 2) |\n",
    "| --- | --- |\n",
    "| 0.77 units | 18.56 units |\n",
    "\n",
    "**Note:** The lower the better, more stable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🏋🏼‍♂️ Improvement in regards to 10 Iteration Test with the Same Policy and Agent but no Training:**\n",
    "\n",
    "| Setting 1 (PPO, Reward Function 1, Hyperparameter Set 1) | Setting 2 (DQN, Reward Function 2, Hyperparameter Set 2) |\n",
    "| --- | --- |\n",
    "| 149.78% | -44.33% |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting 1 achieved a substantially greater average reward of 46.90 points, compared to Setting 2's 26.67 points. This suggests that the PPO algorithm, together with Reward Function 1 and Hyperparameter Set 1, was more successful in teaching the agent to navigate the Highway environment and acquire rewards.\n",
    "\n",
    "Furthermore, Setting 1 demonstrated a lower standard deviation of 0.77 units, indicating a more stable reward performance. On the other hand, Setting 2 exhibited a much higher standard deviation of 18.56 units, suggesting that the DQN algorithm, Reward Function 2, and Hyperparameter Set 2 resulted in more inconsistent and volatile agent behavior.\n",
    "\n",
    "In terms of improvement compared to a 10 iteration test with the same policy and agent but no training, Setting 1 showed a remarkable increase of 149.78% in average reward, showcasing the effectiveness of the PPO algorithm, Reward Function 1, and Hyperparameter Set 1 in training the agent to achieve the project's objective. Setting 2, on the other hand, saw a -44.33% decrease in average reward, showing that the DQN algorithm, Reward Function 2, and Hyperparameter Set 2 were less successful in enhancing the agent's performance when compared to its original state.\n",
    "\n",
    "Overall, although both settings failed to reach the average reward goal of 50 points, based on these results, it can be concluded that Setting 1 (PPO, Reward Function 1, Hyperparameter Set 1) outperformed Setting 2 (DQN, Reward Function 2, Hyperparameter Set 2) in terms of average reward, reward stability, and improvement compared to its initial agent's performance. The PPO algorithm proved to be more effective in training the agent to drive the ambulance faster and safer in the Highway environment, providing valuable insights for the development of autonomous driving algorithms in a real-world scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part X: Limitations and Improvement Opportunities**\n",
    "\n",
    "Reflecting on the main project limitations and future improvement opportunities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⛓🔭 Limitations & Improvement Opportunities:**\n",
    "\n",
    "- **💻 Lack of Computer Power to Perform more Timesteps:** The computational resources available on local machines were limited, which constrained the number of training timesteps that could be performed. As a result, the agent's training might not have reached the optimal number of iterations necessary for achieving higher levels of performance and convergence. \n",
    "\n",
    "To overcome this limitation in future iterations, leveraging ☁ cloud solutions or exploring ways to ◼▪ reduce the input dimensionality could be considered. By utilizing more powerful computing resources or reducing the computational complexity of the simulation input, the training process could be extended to allow for a more comprehensive exploration of the environment, potentially leading to improved results.\n",
    "\n",
    "- **🚑 Too Passive Behavior given the Urgency:** Despite efforts to adjust the reward function to balance safety and speed and modifying the gamma parameter to promote riskier and more short-term point-focused behaviors, the trained agent exhibited a relatively passive driving style, which resulted in slower driving speeds than desired. This limitation implies that the agent may have prioritized safety over urgency, potentially hindering its ability to respond swiftly in emergency situations. \n",
    "\n",
    "To address this limitation in future iterations, additional adjustments could be made to the reward function and hyperparameters. Specifically, fine-tuning the reward weights, especially in terms of right lane and high-speed rewards, could encourage more aggressive and urgent driving behaviors. Furthermore, introducing a new reward component that incentivizes the number of cars rendered behind the ambulance could potentially enhance the agent's sense of urgency and responsiveness in navigating the environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
