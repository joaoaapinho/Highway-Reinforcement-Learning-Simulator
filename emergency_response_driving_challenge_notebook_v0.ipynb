{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning & Reinforcement Learning\n",
    "# **Group Assignment Trial**\n",
    "### **Highway Environment: Emergency Response Driving Challenge**\n",
    "Professor: David Kremer\n",
    "\n",
    "Assignment done by: \n",
    "- Adrian Marino\n",
    "- Felipe Fischel\n",
    "- Gilles Calderón\n",
    "- Jean-Jacon Klat\n",
    "- João André Pinho\n",
    "- Nicholas Dieke\n",
    "- Niccolo Borgato\n",
    "- Max Heilingbrunner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Resolution**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part 0: Imports**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Libraries and Module Imports:**\n",
    "\n",
    "Importing the necessary libraries and modules to manipulate and interact with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part I: Setting UP the Environment**\n",
    "\n",
    "Define the environment that will be used - \"CarRaving-v2\" and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the type of environment.\n",
    "environment_name = 'CarRacing-v2'\n",
    "\n",
    "# Creating an instance of it and setting the render mode as \"human\" so the environment can be seen on video.\n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Action Space.\n",
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The used action space will be continuos and have 3 different actions:\n",
    " - **Steering -** [-1 Full Left, + 1 Full Right]\n",
    " - **Gas -** [0,1 Full Gas]\n",
    " - **Brake -** [0,1 Full Brake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (96, 96, 3), uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The Observation Space will consist of 96x96 RGB (3-channel) pixel frames from the gameplay."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part II: Running a Baseline of Random Episodes**\n",
    "\n",
    "Let the agent take random actions in the environment in order to give a baseline of what to expect from an untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of episodes - number of times the game will play itself either if:\n",
    "#   A. The car visits all tiles.\n",
    "#   B. Goes outside of the playfield and far from the track.\n",
    "episodes = 1\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    \n",
    "    # Reseting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "    \n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0 \n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "        \n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "    \n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As seen, without properly training the agent to appropriatly define the right actions to take the accumulated final scores will be very poor (negative values). \n",
    "\n",
    "In order to improve these the agent must be trained in order to:\n",
    "\n",
    "1. **Gather samples from the gameplay (i.e., running the policy);**\n",
    "2. **Fit a model that estimates the tota final return;**\n",
    "3. **Improving the policy, in order to generate at each step an action with the end goal of maximizing the final acuumulated reward.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part III: Setting up the Agent & Policy**\n",
    "\n",
    "Set up the Agent & Policy using Stable Baselines3 - a famous high-level Python library for reinforcement learning exercises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the environment.\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "# Wraping the environment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Setup the Agent and Policy**\n",
    "\n",
    "Creating an agent using the **Proximal Policy Optimization** algorithm.\n",
    "\n",
    "This algorithm was built in order to optimize the decisions made by an agent in a given environment in order to maximize the notion of cumulative reward. The idea behind consists of taking steps in the direction that improves the policy, but not too large ones that might harm the performance. In other words, it seeks an optimal policy that is close to the current policy. \n",
    "\n",
    "Mathematically, PPO aims to find the optimal policy π(θ) that maximizes the objective:\n",
    "<center><img src=\"images/ppo_formula.png\"/></center>\n",
    "\n",
    "The main innovation of the this algorithm is the introduction of a \"Clipping\" mechanism that penalizes changes in the policy that deviate too much from the curren policy.\n",
    "\n",
    "In terms of the Policy, \"CNNPolicy\" - a policy architecture option that is especially good at processing grid-like data (i.e., images) was chosen. This CNN, particularly good for game environments, will take in the image-based observation from the environment, process it through its layers to detect important features, and then output an action (or a distribution over actions) that the agent should take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = PPO(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1,\n",
    "            # Defining the discount factor for future rewards (how much the agent cares about rewards in the distance future relative to immediate ones).\n",
    "            gamma=0.99,  \n",
    "            # Defining the trade-off between exploration (trying out new actions) and exploitation (sticking to what's known to work)\n",
    "            ent_coef=0.01, \n",
    "            # Defining the learning rate for the optimizer.\n",
    "            learning_rate=0.00025, \n",
    "            # Defining the value function coefficient - scaling factor to change the contribution of the value function loss to the total loss function.\n",
    "            vf_coef=0.5, \n",
    "            # Defining the maximum value for the gradient clipping.\n",
    "            max_grad_norm=0.5,\n",
    "            # Defining the trade-off between variance and bias - technique to reduce variance.\n",
    "            gae_lambda=0.95,\n",
    "            # Defining the clipping parameter for the policy update - how much the new policy can deviate from the old policy during each update.\n",
    "            clip_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the Agent and the Policy.\n",
    "model = PPO(\"CnnPolicy\", \n",
    "            # Defining the environment the model will use.\n",
    "            env, \n",
    "            # Defining the level of detail of training logs.\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part IV: Initial Training**\n",
    "\n",
    "Initially train the model for a small number of steps to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the short model training path.\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_15_Driving_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the short model timesteps - 15.\n",
    "#model.learn(total_timesteps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model.\n",
    "#model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model.\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to see if it was properly saved.\n",
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part V: Initial Evaluation**\n",
    "\n",
    "Evaluating the performance of the agent after being trained with the short timestepped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the model.\n",
    "evaluate_policy(model, env, n_eval_episodes=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** Although we can see some improvements in terms of the actions that the car takes, the agent is definetly not trained as most of its time is spent on grass instead of trying to complete the track. Given such poor generated scores, the hypothesis that can be raised is that with a larger amount of training steps, the agent will be able to learn a better policy in order to take actions that will maximize the accumulated score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VI: Training Stage**\n",
    "\n",
    "Now that the training process is understood and after confirming the poor results from only training the agent on 15 timesteps, the model will be trained for a longer period of time to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the long model training path.\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_100_Driving_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the long model on 100K timesteps.\n",
    "#model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model.\n",
    "#model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the model.\n",
    "#del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VII: Testing and Evaluation Stages**\n",
    "\n",
    "Evaluating the performance of the testing performed with the long timestepped model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Define the Environment:**\n",
    "Defining the type of environment - 'CarRacing-v2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the type of environment.\n",
    "environment_name = 'CarRacing-v2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Create an Environment Instance and Wrap it:**\n",
    "\n",
    "Creating a new environment instance and wrapping it with the DummyVecEnc class from Stable Baselines3. This will turn the environment into a vectorized environment that allows for multiples copies to be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of it and setting the render mode as \"human\" so the environment can be seen on video.\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "# Wraping the nevironment to make it compatible with Stable Baselines3 library - which expects a vectorized environment.\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Load the Model:**\n",
    "\n",
    "Loading the longer trained model from its orginal path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_100_Driving_model')\n",
    "model = PPO.load(ppo_path, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Evaluation:**\n",
    "\n",
    "This step is done to measure the performance of your trained agent. Evaluation gives us an estimate of how well the agent will perform and can be used to compare different models or configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the longer trained model.\n",
    "evalue = evaluate_policy(model, env, n_eval_episodes=2, render=True)\n",
    "print(evalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **• Model Testing:**\n",
    "\n",
    "This step is done as a final assessment of the agent, in order to check how it would operate in the real world given the model that it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running multiple episodes and record the scores and time steps.\n",
    "episodes = 3\n",
    "scores_array = []\n",
    "timestep_arr = []\n",
    "\n",
    "# Starting a loop to play the given number of episodes.\n",
    "for episode in range(1, episodes+1):\n",
    "    # Resetting the environment to get the initial state - car starts at rest in the center of the road.\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Flagging to indicate whether the episode is done or not (i.e., whether the terminal state has been reached).\n",
    "    done = False\n",
    "\n",
    "    # Initializing the score for the episode to 0.\n",
    "    score = 0\n",
    "\n",
    "    # Initializing the timestep as 0.\n",
    "    timestep = 0\n",
    "    \n",
    "    # Starting a loop that continues until the episode is done.\n",
    "    while not done:\n",
    "\n",
    "        # Choosing an action randomly from the action space of the environment.\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Taking the chosen action in the environment, and get the next state, reward, done flag, and additional info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Adding the reward for this step to the score.\n",
    "        score += reward\n",
    "\n",
    "        # Incrementing the timesteps.\n",
    "        timestep += 1\n",
    "        \n",
    "        # Render the environment.\n",
    "        env.render()\n",
    "    \n",
    "    # Saving both the scores and timesteps.\n",
    "    scores_array.append(score)\n",
    "    timestep_arr.append(timestep)\n",
    "\n",
    "    # Printing out the score for the episode after its conclusion.\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))\n",
    "\n",
    "# CLosing the environment.\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Part VIII: Score Analysis**\n",
    "\n",
    "Plotting the scores and time steps for each episode to analyze the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the scores and timestep per episode.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_array)+1), scores_array, label=\"Score\", color='red')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,)\n",
    "plt.xticks(np.arange(1, len(scores_array)+1))\n",
    "plt.xlabel('Episodes #')\n",
    "plt.title(\"Scores over Episodes\", fontweight='bold' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** As observable, with the longer trainedstepped model the results far exceeded the ones from the first testing stages. Despite the better score values, some of irregularity on these still needs to be addressed, hence for now the challenge wasn't fully successful given that none of the runs achieved the necessary 900 points in order to pass it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
